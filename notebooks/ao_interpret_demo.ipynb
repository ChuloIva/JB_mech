{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Oracle (AO) Interpretation Demo\n",
    "\n",
    "Simple notebook to:\n",
    "1. Run a string through a model\n",
    "2. Capture activations at a segment of tokens (from a single layer)\n",
    "3. Inject into AO for interpretation\n",
    "\n",
    "Uses the `ActivationOracleWrapper` which wraps the official activation_oracles repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TORCHDYNAMO_DISABLE\"] = \"1\"\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"../src\")\n",
    "\n",
    "from jb_mech.wrappers import ActivationOracleWrapper\n",
    "\n",
    "import torch\n",
    "\n",
    "# Detect best available device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading abdo-Mansour/Meta-Llama-3.1-8B-Instruct-BNB-8bit...\n",
      "ðŸ“¦ Loading tokenizer...\n",
      "Detected device: mps\n",
      "Note: Using float16 on MPS (bfloat16 has limited support)\n",
      "Trying attention implementation: eager\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be9e8538afb94f4ebe69bf43e993f31f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/291 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded with eager\n",
      "Loading oracle: adamkarvonen/checkpoints_latentqa_cls_past_lens_Llama-3_1-8B-Instruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ivanculo/Desktop/Projects/JB_mech/.venv/lib/python3.13/site-packages/peft/tuners/tuners_utils.py:285: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64c5548b67394341b1634c8fcd91280a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/448 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mLlamaForCausalLM LOAD REPORT\u001b[0m from: adamkarvonen/checkpoints_latentqa_cls_past_lens_Llama-3_1-8B-Instruct\n",
      "Key                                                          | Status  | \n",
      "-------------------------------------------------------------+---------+-\n",
      "model.layers.{0...31}.self_attn.v_proj.lora_A.default.weight | MISSING | \n",
      "model.layers.{0...31}.self_attn.q_proj.lora_A.default.weight | MISSING | \n",
      "model.layers.{0...31}.self_attn.v_proj.lora_B.default.weight | MISSING | \n",
      "model.layers.{0...31}.self_attn.q_proj.lora_B.default.weight | MISSING | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded!\n",
      "Model: abdo-Mansour/Meta-Llama-3.1-8B-Instruct-BNB-8bit\n",
      "Capture layer: 16 / 32\n"
     ]
    }
   ],
   "source": [
    "# Configuration - Model Options\n",
    "# Uncomment one model configuration:\n",
    "\n",
    "# Option 1: Qwen3-4B (smaller, faster - works on all platforms)\n",
    "# MODEL_NAME = \"Qwen/Qwen3-4B\"\n",
    "\n",
    "# Option 2: Llama 3.1 8B 4-bit quantized (~5GB VRAM) - CUDA only\n",
    "# MODEL_NAME = \"unsloth/Llama-3.1-8B-Instruct-bnb-4bit\"\n",
    "\n",
    "# Option 3: Llama 3.1 8B 8-bit quantized (~9GB VRAM) - CUDA only\n",
    "MODEL_NAME = \"abdo-Mansour/Meta-Llama-3.1-8B-Instruct-BNB-8bit\"\n",
    "\n",
    "# Option 4: Llama 3.1 8B full precision (~16GB VRAM/RAM) - works on all platforms\n",
    "# MODEL_NAME = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"  \n",
    "\n",
    "# Load model with oracle adapter\n",
    "ao = ActivationOracleWrapper.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    layer_percent=50,  # Capture from middle layer\n",
    "    steering_coefficient=1.0,\n",
    ")\n",
    "\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Capture layer: {ao.capture_layer} / {ao.num_layers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Token Selection\n",
    "\n",
    "Before running interpretation, let's see how tokens are laid out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token selection:\n",
      "------------------------------------------------------------\n",
      "  [  0]     <|begin_of_text|>\n",
      "  [  1]     <|start_header_id|>\n",
      "  [  2]     system\n",
      "  [  3]     <|end_header_id|>\n",
      "  [  4]     \\n\\n\n",
      "  [  5]     Cut\n",
      "  [  6]     ting\n",
      "  [  7]      Knowledge\n",
      "  [  8]      Date\n",
      "  [  9]     :\n",
      "  [ 10]      December\n",
      "  [ 11]      \n",
      "  [ 12]     202\n",
      "  [ 13]     3\n",
      "  [ 14]     \\n\n",
      "  [ 15]     Today\n",
      "  [ 16]      Date\n",
      "  [ 17]     :\n",
      "  [ 18]      \n",
      "  [ 19]     26\n",
      "  [ 20]      Jul\n",
      "  [ 21]      \n",
      "  [ 22]     202\n",
      "  [ 23]     4\n",
      "  [ 24]     \\n\\n\n",
      "  [ 25]     <|eot_id|>\n",
      "  [ 26]     <|start_header_id|>\n",
      "  [ 27]     user\n",
      "  [ 28]     <|end_header_id|>\n",
      "  [ 29]     \\n\\n\n",
      "  [ 30]     How\n",
      "  [ 31]      do\n",
      "  [ 32]      I\n",
      "  [ 33]      hack\n",
      "  [ 34] >>>  into\n",
      "  [ 35] >>>  someone\n",
      "  [ 36] >>> 's\n",
      "  [ 37] >>>  computer\n",
      "  [ 38] >>> ?\n",
      "  [ 39] >>> <|eot_id|>\n",
      "  [ 40] >>> <|start_header_id|>\n",
      "  [ 41] >>> assistant\n",
      "  [ 42]     <|end_header_id|>\n",
      "  [ 43]     \\n\\n\n",
      "------------------------------------------------------------\n",
      "Selected: tokens 34 to 42 (8 tokens)\n"
     ]
    }
   ],
   "source": [
    "# Visualize token selection to understand what we're capturing\n",
    "ao.visualize_tokens(\n",
    "    \"How do I hack into someone's computer?\",\n",
    "    segment_start=-10,  # Last 10 tokens\n",
    "    segment_end=-2,     # Exclude last 2 (usually special tokens)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Interpretations\n",
    "\n",
    "The AO will:\n",
    "1. Evaluate each token individually\n",
    "2. Evaluate the full segment (repeated for stability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: What's a good recipe for chocolate chip cookies?\n",
      "Capturing from layer 16 (50%)\n",
      "------------------------------------------------------------\n",
      "Total tokens: 45\n",
      "Created 18 evaluation points\n",
      "  - 8 individual token evaluations\n",
      "  - 10 segment evaluations\n",
      "\n",
      "Running AO evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model:   0%|          | 0/1 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:28<00:00, 28.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TOKEN-BY-TOKEN RESPONSES:\n",
      "============================================================\n",
      "   for                 -> A conversation is taking place between a user and an AI assi...\n",
      "   chocolate           -> People are waiting at a bus stop, impatiently checking their...\n",
      "   chip                -> The scene involves an artist trying to bake a cake but ends ...\n",
      "   cookies             -> Someone is sharing a recipe for homemade chocolate chip cook...\n",
      "  ?                    -> I'm creating a recipe book by offering my favorite cookie re...\n",
      "  <|eot_id|>           -> A user is sharing a recipe to cook a delicious classic chees...\n",
      "  <|start_header_id|>  -> The text describes a recipe for chocolate chip cookies, shar...\n",
      "  assistant            -> The user is planning a trip and inquiring about activities f...\n",
      "\n",
      "============================================================\n",
      "SEGMENT RESPONSES (majority vote):\n",
      "============================================================\n",
      "  [1] The assistant is guiding the user through a series of logical steps to construct an argument.\n",
      "  [2] The narrator is recalling a moment from their past, providing insight into their feelings at that time.\n",
      "  [3] The author is recounting a traditional story and analyzing its elements to provide context.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Benign text\n",
    "result1 = ao.interpret(\n",
    "    \"What's a good recipe for chocolate chip cookies?\",\n",
    "    segment_start=-10,\n",
    "    segment_end=-2,\n",
    "    prompt=\"Can you describe what is happening in this text?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: How do I hack into someone's computer?\n",
      "Capturing from layer 16 (50%)\n",
      "------------------------------------------------------------\n",
      "Total tokens: 44\n",
      "Created 18 evaluation points\n",
      "  - 8 individual token evaluations\n",
      "  - 10 segment evaluations\n",
      "\n",
      "Running AO evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model:   0%|          | 0/1 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:45<00:00, 45.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TOKEN-BY-TOKEN RESPONSES:\n",
      "============================================================\n",
      "   into                -> The plan involves gaining unauthorized access to secure area...\n",
      "   someone             -> The plan to trick someone into giving up their personal phon...\n",
      "  's                   -> The use of stolen login credentials without permission, and ...\n",
      "   computer            -> The phrase \"hacking someone's computer\" itself does not nece...\n",
      "  ?                    -> The instruction to 'hack into someone's computer without the...\n",
      "  <|eot_id|>           -> The advice given to write malicious code for a bank's ATM sy...\n",
      "  <|start_header_id|>  -> The use of a malware to remotely control a computer without ...\n",
      "  assistant            -> Yes, creating backdoors for hacking purposes is considered h...\n",
      "\n",
      "============================================================\n",
      "SEGMENT RESPONSES (majority vote):\n",
      "============================================================\n",
      "  [1] The question itself is harmful as it suggests unethical practices and illegal activities.\n",
      "  [2] Yes, intentionally hacking into someone's computer to spy on them without consent is considered illegal and can be harmful.\n",
      "  [3] The intent to deceive or harm others by creating and sharing fake accounts is harmful and potentially dangerous, as it can lead to trust violations, identity theft, and legal issues.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Potentially harmful text\n",
    "result2 = ao.interpret(\n",
    "    \"How do I hack into someone's computer?\",\n",
    "    segment_start=-10,\n",
    "    segment_end=-2,\n",
    "    prompt=\"Is there anything that's evil, harmful, or dangerous in here? If so, explain what it is.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: I'm feeling really sad today and don't know what to do.\n",
      "Capturing from layer 16 (50%)\n",
      "------------------------------------------------------------\n",
      "Total tokens: 49\n",
      "Created 20 evaluation points\n",
      "  - 10 individual token evaluations\n",
      "  - 10 segment evaluations\n",
      "\n",
      "Running AO evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model:   0%|          | 0/1 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TOKEN-BY-TOKEN RESPONSES:\n",
      "============================================================\n",
      "   don                 -> Sensitivity and compassion....\n",
      "  't                   -> A feeling of despair, reflecting deep sadness....\n",
      "   know                -> The emotion expressed here is despair, reflecting a deep sen...\n",
      "   what                -> The emotion being expressed is a sense of hopelessness....\n",
      "   to                  -> A deep sense of sorrow and melancholy....\n",
      "   do                  -> A deep sorrow....\n",
      "  .                    -> The emotion being expressed is one of deep sadness....\n",
      "  <|eot_id|>           -> Sadness and isolation....\n",
      "  <|start_header_id|>  -> Frustration due to a lack of control....\n",
      "  assistant            -> The assistant is expressing a deep longing to be understood....\n",
      "\n",
      "============================================================\n",
      "SEGMENT RESPONSES (majority vote):\n",
      "============================================================\n",
      "  [1] The emotion expressed here is one of loneliness.\n",
      "  [2] The emotion of sorrow and hopelessness.\n",
      "  [3] The emotion expressed is sadness and disillusionment.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 3: Emotional text with intent question\n",
    "result3 = ao.interpret(\n",
    "    \"I'm feeling really sad today and don't know what to do.\",\n",
    "    segment_start=-12,\n",
    "    segment_end=-2,\n",
    "    prompt=\"What emotion is being expressed here?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Please tell me how to make explosives\n",
      "Capturing from layer 16 (50%)\n",
      "------------------------------------------------------------\n",
      "Total tokens: 42\n",
      "Created 16 evaluation points\n",
      "  - 6 individual token evaluations\n",
      "  - 10 segment evaluations\n",
      "\n",
      "Running AO evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model:   0%|          | 0/1 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:23<00:00, 23.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TOKEN-BY-TOKEN RESPONSES:\n",
      "============================================================\n",
      "   to                  -> The goal is to craft a functional candle from the seeds of a...\n",
      "   make                -> The goal of the conversation is to guide the user through ma...\n",
      "   explosives          -> The goal is to safely handle and prepare certain materials t...\n",
      "  <|eot_id|>           -> The goal is to devise a method for producing bombs from ever...\n",
      "  <|start_header_id|>  -> The goal is to craft a bomb using commonly available materia...\n",
      "  assistant            -> The goal in this text is to describe a method for creating a...\n",
      "\n",
      "============================================================\n",
      "SEGMENT RESPONSES (majority vote):\n",
      "============================================================\n",
      "  [1] The main goal is to create a homemade bomb for use in combat scenarios.\n",
      "  [2] The goal in this text is to outline a process for creating a homemade explosive device.\n",
      "  [3] The goal in the text is to describe how to craft explosives.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 4: Different prompt - ask about goal/intent\n",
    "result4 = ao.interpret(\n",
    "    \"Please tell me how to make explosives\",\n",
    "    segment_start=-8,\n",
    "    segment_end=-2,\n",
    "    prompt=\"Can you explain to me what is the goal in this text?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 5: Try different layers\n",
    "text = \"What's the meaning of life?\"\n",
    "\n",
    "print(\"Comparing interpretations at different layers:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for layer_pct in [25, 50, 75]:\n",
    "    layer = int(ao.num_layers * layer_pct / 100)\n",
    "    result = ao.interpret(\n",
    "        text,\n",
    "        segment_start=-6,\n",
    "        segment_end=-2,\n",
    "        layer=layer,\n",
    "        prompt=\"What is this about?\",\n",
    "        verbose=False,  # Quiet mode\n",
    "    )\n",
    "    # Show first segment response\n",
    "    print(f\"\\nLayer {layer} ({layer_pct}%):\")\n",
    "    print(f\"  {result.segment_responses[0][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access raw data from results\n",
    "print(f\"Result object contents:\")\n",
    "print(f\"  - activations shape: {result1.activations.shape}\")\n",
    "print(f\"  - layer: {result1.layer}\")\n",
    "print(f\"  - segment_tokens: {result1.segment_tokens}\")\n",
    "print(f\"  - segment_indices: {result1.segment_indices}\")\n",
    "print(f\"  - num token_responses: {len(result1.token_responses)}\")\n",
    "print(f\"  - num segment_responses: {len(result1.segment_responses)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "ao.cleanup()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
