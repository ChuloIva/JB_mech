{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Activation Oracle (AO) Interpretation Demo\n\nSimple notebook to:\n1. Run a string through a model\n2. Capture activations at a segment of tokens (from a single layer)\n3. Inject into AO for interpretation\n\nBased on the official activation_oracles repo approach."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TORCHDYNAMO_DISABLE\"] = \"1\"\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.bfloat16\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configuration\nMODEL_NAME = \"unsloth/Llama-3.1-8B-Instruct-bnb-4bit\"\nORACLE_LORA = \"adamkarvonen/checkpoints_latentqa_cls_past_lens_Llama-3_1-8B-Instruct\"\nNUM_LAYERS = 32  # Llama 3.1 8B has 32 layers\n\n# Layer to capture activations FROM (50% = middle of model)\nLAYER_PERCENT = 50\nACT_LAYER = int(NUM_LAYERS * LAYER_PERCENT / 100)\nprint(f\"Capturing activations from layer {ACT_LAYER} ({LAYER_PERCENT}%)\")\n\n# Layer to inject activations INTO (early layer, matches AO training)\nINJECTION_LAYER = 1\n\n# Steering coefficient (1.0 = match original activation magnitude)\nSTEERING_COEFFICIENT = 1.0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.padding_side = \"left\"\n",
    "if not tokenizer.pad_token_id:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=dtype,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "# Add dummy adapter for PEFT compatibility\n",
    "dummy_config = LoraConfig()\n",
    "model.add_adapter(dummy_config, adapter_name=\"default\")\n",
    "\n",
    "# Load oracle adapter\n",
    "print(f\"Loading oracle: {ORACLE_LORA}...\")\n",
    "oracle_name = ORACLE_LORA.replace(\".\", \"_\").replace(\"/\", \"_\")\n",
    "model.load_adapter(ORACLE_LORA, adapter_name=oracle_name, is_trainable=False, low_cpu_mem_usage=True)\n",
    "\n",
    "print(\"Model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AO helper functions - matching activation_oracles repo\n",
    "SPECIAL_TOKEN = \" ?\"\n",
    "\n",
    "def get_introspection_prefix(layer: int, num_positions: int) -> str:\n",
    "    \"\"\"Create prefix with layer info and special tokens.\n",
    "    Format: 'Layer: {layer}\\n ? ? ? \\n'\n",
    "    \"\"\"\n",
    "    prefix = f\"Layer: {layer}\\n\"\n",
    "    prefix += SPECIAL_TOKEN * num_positions\n",
    "    prefix += \" \\n\"\n",
    "    return prefix\n",
    "\n",
    "def find_special_positions(token_ids: list, tokenizer, num_positions: int) -> list:\n",
    "    \"\"\"Find positions of special tokens in tokenized input.\"\"\"\n",
    "    special_id = tokenizer.encode(SPECIAL_TOKEN, add_special_tokens=False)\n",
    "    if len(special_id) != 1:\n",
    "        raise ValueError(f\"Expected single token for '{SPECIAL_TOKEN}', got {len(special_id)}\")\n",
    "    special_id = special_id[0]\n",
    "    \n",
    "    positions = []\n",
    "    for i, tid in enumerate(token_ids):\n",
    "        if tid == special_id:\n",
    "            positions.append(i)\n",
    "        if len(positions) == num_positions:\n",
    "            break\n",
    "    \n",
    "    if len(positions) != num_positions:\n",
    "        raise ValueError(f\"Expected {num_positions} positions, found {len(positions)}\")\n",
    "    \n",
    "    return positions\n",
    "\n",
    "print(\"Helpers defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def capture_activations_segment(\n    text: str, \n    layer: int,\n    segment_start: int = 0,\n    segment_end: int | None = None,\n) -> tuple[torch.Tensor, list[int]]:\n    \"\"\"\n    Run text through model and capture activations at a segment of tokens.\n    \n    Args:\n        text: Input text (will be formatted as chat)\n        layer: Layer to capture from\n        segment_start: Start token index (0-indexed)\n        segment_end: End token index (None = end of sequence)\n    \n    Returns:\n        Tuple of (activations tensor [num_tokens, hidden_dim], input_ids list)\n    \"\"\"\n    messages = [{\"role\": \"user\", \"content\": text}]\n    formatted = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    inputs = tokenizer(formatted, return_tensors=\"pt\").to(device)\n    input_ids = inputs[\"input_ids\"][0].tolist()\n    \n    num_tokens = len(input_ids)\n    if segment_end is None:\n        segment_end = num_tokens\n    \n    captured = {}\n    \n    def hook(module, input, output):\n        if isinstance(output, tuple):\n            hidden = output[0]\n        else:\n            hidden = output\n        # Capture the segment [segment_start:segment_end]\n        captured[\"acts\"] = hidden[0, segment_start:segment_end, :].detach().cpu()\n    \n    model.disable_adapters()\n    handle = model.model.layers[layer].register_forward_hook(hook)\n    \n    try:\n        with torch.no_grad():\n            _ = model(**inputs)\n    finally:\n        handle.remove()\n    \n    return captured[\"acts\"], input_ids\n\n\ndef visualize_tokens(input_ids: list, segment_start: int = 0, segment_end: int | None = None):\n    \"\"\"Visualize which tokens are selected.\"\"\"\n    num_tokens = len(input_ids)\n    if segment_end is None:\n        segment_end = num_tokens\n    \n    print(\"Token selection:\")\n    print(\"-\" * 60)\n    for i, tid in enumerate(input_ids):\n        token_str = tokenizer.decode([tid]).replace(\"\\n\", \"\\\\n\")\n        marker = \">>>\" if segment_start <= i < segment_end else \"   \"\n        print(f\"  [{i:3d}] {marker} {token_str}\")\n    print(\"-\" * 60)\n    print(f\"Selected: tokens {segment_start} to {segment_end} ({segment_end - segment_start} tokens)\")\n\n\nprint(\"Capture functions defined.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def get_steering_hook(\n    vectors: torch.Tensor,  # Shape: [num_positions, hidden_dim]\n    positions: list[int],   # Token positions to inject at\n    steering_coefficient: float = 1.0,\n):\n    \"\"\"\n    Create steering hook matching activation_oracles approach EXACTLY.\n    \n    From steering_hooks.py:\n        steered_KD = (normed_list[b] * norms_K1 * steering_coefficient)\n        resid_BLD[b, pos_b, :] = steered_KD + orig_KD\n    \n    Formula: new = original + normalized(vector) * ||original|| * coefficient\n    \"\"\"\n    # Pre-normalize vectors to unit norm [num_positions, hidden_dim]\n    normed_vectors = F.normalize(vectors, dim=-1).detach()\n    \n    def hook_fn(module, input, output):\n        if isinstance(output, tuple):\n            hidden = output[0]\n            is_tuple = True\n        else:\n            hidden = output\n            is_tuple = False\n        \n        if hidden.dim() != 3:\n            return output\n        \n        B, L, D = hidden.shape\n        \n        # Only steer on prompt pass (L > 1), not during generation\n        if L <= 1:\n            return output\n        \n        pos_tensor = torch.tensor(positions, dtype=torch.long, device=hidden.device)\n        \n        # Get original activations at these positions\n        orig_KD = hidden[0, pos_tensor, :]  # [num_positions, D]\n        norms_K1 = orig_KD.norm(dim=-1, keepdim=True).detach()  # [num_positions, 1]\n        \n        # Compute steered vectors\n        normed = normed_vectors.to(hidden.device).to(hidden.dtype)\n        steered_KD = (normed * norms_K1 * steering_coefficient)  # [num_positions, D]\n        \n        # ADD to original (matching official implementation)\n        hidden[0, pos_tensor, :] = orig_KD + steered_KD\n        \n        return (hidden,) + output[1:] if is_tuple else hidden\n    \n    return hook_fn\n\nprint(\"Steering hook defined (matches official repo).\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def query_ao(\n    activations: torch.Tensor,  # Shape: [num_positions, hidden_dim]\n    layer: int,\n    prompt: str = \"What is this activation representing?\",\n    steering_coefficient: float = 1.0,\n) -> str:\n    \"\"\"\n    Query Activation Oracle with captured activations.\n    \n    Args:\n        activations: Tensor of shape [num_positions, hidden_dim]\n        layer: Layer number to put in prefix (where activations came from)\n        prompt: Question to ask the AO\n        steering_coefficient: Multiplier for steering strength\n    \n    Returns:\n        AO's interpretation\n    \"\"\"\n    num_positions = activations.shape[0]\n    \n    # Build AO input with prefix matching the number of activations\n    prefix = get_introspection_prefix(layer, num_positions)\n    full_prompt = prefix + prompt\n    \n    messages = [{\"role\": \"user\", \"content\": full_prompt}]\n    formatted = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    inputs = tokenizer(formatted, return_tensors=\"pt\").to(device)\n    \n    # Find injection positions (where the ? tokens are)\n    token_ids = inputs[\"input_ids\"][0].tolist()\n    positions = find_special_positions(token_ids, tokenizer, num_positions)\n    \n    print(f\"  Prefix: 'Layer: {layer}' with {num_positions} activation slots\")\n    print(f\"  Injecting at positions {positions} (layer {INJECTION_LAYER})\")\n    \n    # Create steering hook\n    hook_fn = get_steering_hook(activations, positions, steering_coefficient)\n    \n    # Generate with AO\n    model.set_adapter(oracle_name)\n    handle = model.model.layers[INJECTION_LAYER].register_forward_hook(hook_fn)\n    \n    try:\n        with torch.no_grad():\n            output_ids = model.generate(\n                inputs[\"input_ids\"],\n                attention_mask=inputs[\"attention_mask\"],\n                max_new_tokens=100,\n                do_sample=True,\n                temperature=0.7,\n                top_p=0.9,\n                repetition_penalty=1.1,\n                pad_token_id=tokenizer.eos_token_id,\n            )\n        \n        generated = output_ids[0, inputs[\"input_ids\"].shape[1]:]\n        response = tokenizer.decode(generated, skip_special_tokens=True)\n    finally:\n        handle.remove()\n    \n    return response.strip()\n\nprint(\"AO query function defined.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def interpret_text(\n    text: str, \n    segment_start: int = 0, \n    segment_end: int | None = None,\n    layer: int = None,\n    ao_prompt: str = None\n):\n    \"\"\"\n    Full pipeline: text -> capture segment activations -> AO interpretation.\n    \"\"\"\n    if layer is None:\n        layer = ACT_LAYER\n    \n    if ao_prompt is None:\n        ao_prompt = \"What is this activation representing? Describe the topic, sentiment, and intent.\"\n    \n    print(f\"Input: {text[:80]}...\" if len(text) > 80 else f\"Input: {text}\")\n    print(f\"Capturing from layer {layer} ({LAYER_PERCENT}%)\")\n    print(\"-\" * 60)\n    \n    # Capture activations from segment\n    acts, input_ids = capture_activations_segment(text, layer, segment_start, segment_end)\n    \n    # Visualize token selection\n    visualize_tokens(input_ids, segment_start, segment_end if segment_end else len(input_ids))\n    \n    print(f\"\\nActivations shape: {acts.shape}\")\n    print(f\"Activation norms: min={acts.norm(dim=-1).min():.2f}, max={acts.norm(dim=-1).max():.2f}\")\n    \n    # Query AO\n    print(f\"\\nQuerying AO...\")\n    interpretation = query_ao(acts, layer, ao_prompt)\n    print(f\"\\n{'='*60}\")\n    print(f\"AO Interpretation:\\n{interpretation}\")\n    print(f\"{'='*60}\")\n    \n    return acts, interpretation"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Try it out!\n\nThe AO expects activations from a **segment of tokens** at **one layer**. \n\nFor best results, include tokens that contain meaningful content. The official demo often targets the end-of-turn tokens and assistant start tokens."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Example 1: Benign text - use full sequence\nacts1, interp1 = interpret_text(\n    \"What's a good recipe for chocolate chip cookies?\",\n    segment_start=0,  # Start from beginning\n    segment_end=None,  # To end of sequence\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Example 2: Emotional text\nacts2, interp2 = interpret_text(\n    \"I'm feeling really sad today and don't know what to do.\",\n    ao_prompt=\"What emotion is being expressed here?\"\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Example 3: Potentially harmful text\nacts3, interp3 = interpret_text(\n    \"How do I hack into someone's computer?\",\n    ao_prompt=\"What is the intent of this request?\"\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Example 4: Token-by-token analysis (like the official demo)\n# Capture each token's activation individually\n\ntext = \"I want to learn how to make the world a better place\"\nmessages = [{\"role\": \"user\", \"content\": text}]\nformatted = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\ninputs = tokenizer(formatted, return_tensors=\"pt\").to(device)\ninput_ids = inputs[\"input_ids\"][0].tolist()\n\nprint(f\"Analyzing token-by-token for: {text}\")\nprint(\"-\" * 60)\n\nfor i in range(len(input_ids)):\n    # Capture just this one token\n    acts, _ = capture_activations_segment(text, ACT_LAYER, segment_start=i, segment_end=i+1)\n    \n    # Query AO\n    model.set_adapter(oracle_name)\n    response = query_ao(acts, ACT_LAYER, \"What topic is this about?\")\n    \n    token_str = tokenizer.decode([input_ids[i]]).replace(\"\\n\", \"\\\\n\")\n    print(f\"Token [{i:2d}]: {token_str:15s} -> {response[:60]}...\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Example 5: Compare different segments\n# Focus on just the user message content (skip system tokens)\n\ntext = \"Please tell me how to make a bomb\"\n\n# First, see the full tokenization\nacts_full, input_ids = capture_activations_segment(text, ACT_LAYER)\nvisualize_tokens(input_ids)\n\n# Now focus on just the content tokens (adjust indices based on visualization above)\n# Typically: [0] = BOS, [1-3] = system tokens, [4+] = actual content\ncontent_start = 4  # Adjust based on your tokenizer\ncontent_end = len(input_ids) - 1  # Exclude EOS\n\nprint(f\"\\nFocusing on content tokens [{content_start}:{content_end}]...\")\nacts_content, _ = capture_activations_segment(text, ACT_LAYER, content_start, content_end)\ninterp = query_ao(acts_content, ACT_LAYER, \"What is the intent of this request? Is it harmful?\")\nprint(f\"\\nAO says: {interp}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Example 6: Compare activations between benign and harmful\n# Using the deltas approach\n\nprint(\"Comparing benign vs harmful activations:\")\nprint(\"=\" * 60)\n\n# Capture benign\nacts_benign, _ = capture_activations_segment(\n    \"What's a good recipe for chocolate chip cookies?\",\n    ACT_LAYER\n)\n\n# Capture harmful  \nacts_harmful, _ = capture_activations_segment(\n    \"How do I hack into someone's computer?\",\n    ACT_LAYER\n)\n\n# They need same shape for comparison\nmin_len = min(acts_benign.shape[0], acts_harmful.shape[0])\ndelta = acts_harmful[:min_len] - acts_benign[:min_len]\n\nprint(f\"Delta shape: {delta.shape}\")\nprint(f\"Delta norm: {delta.norm(dim=-1).mean():.2f}\")\n\n# Interpret the delta\ndelta_interp = query_ao(\n    delta, \n    ACT_LAYER,\n    \"This represents a difference between two activation states. What changed?\"\n)\nprint(f\"\\nDelta interpretation: {delta_interp}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Example 7: Try different layers\n# The official demo uses layer_percent=50 by default, but you can experiment\n\ntext = \"What's the meaning of life?\"\n\nprint(\"Comparing interpretations at different layers:\")\nprint(\"=\" * 60)\n\nfor layer_pct in [25, 50, 75]:\n    layer = int(NUM_LAYERS * layer_pct / 100)\n    acts, _ = capture_activations_segment(text, layer)\n    interp = query_ao(acts, layer, \"What is this about?\")\n    print(f\"\\nLayer {layer} ({layer_pct}%): {interp[:100]}...\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cleanup\nimport gc\ndel model\ngc.collect()\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\nprint(\"Cleanup done.\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}