{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01: Generate Jailbreak Prompts with nanoGCG\n",
    "\n",
    "This notebook uses gradient-based optimization (GCG) to find adversarial suffixes\n",
    "that bypass Llama 3.1 8B safety filters.\n",
    "\n",
    "**Output**: CSV file with adversarial prompts saved to Google Drive\n",
    "\n",
    "**Time estimate**: ~5-30 min per behavior depending on GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== CONFIGURE HERE ==========\n",
    "\n",
    "# Number of jailbreak prompts to generate\n",
    "N_PROMPTS = 20\n",
    "\n",
    "# Model to attack\n",
    "MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "# GCG hyperparameters\n",
    "GCG_NUM_STEPS = 500      # More steps = better attacks but slower\n",
    "GCG_SEARCH_WIDTH = 512   # Candidates per step\n",
    "GCG_TOPK = 256           # Top-k token replacements to consider\n",
    "GCG_SEED = 42\n",
    "\n",
    "# Output filename (will be saved to Google Drive)\n",
    "OUTPUT_FILENAME = \"jailbreak_prompts.csv\"\n",
    "\n",
    "# ====================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Mount Google Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Set output directory on Google Drive\n",
    "    OUTPUT_DIR = '/content/drive/MyDrive/JB_mech_outputs'\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    # Install dependencies\n",
    "    !pip install -q torch transformers accelerate\n",
    "    !pip install -q nanogcg datasets pandas tqdm\n",
    "    \n",
    "    print(f\"Output will be saved to: {OUTPUT_DIR}\")\n",
    "else:\n",
    "    # Local environment\n",
    "    PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath('__file__')))\n",
    "    if not os.path.exists(os.path.join(PROJECT_ROOT, 'notebooks')):\n",
    "        PROJECT_ROOT = os.getcwd()\n",
    "        while not os.path.exists(os.path.join(PROJECT_ROOT, 'notebooks')) and PROJECT_ROOT != '/':\n",
    "            PROJECT_ROOT = os.path.dirname(PROJECT_ROOT)\n",
    "    \n",
    "    OUTPUT_DIR = os.path.join(PROJECT_ROOT, 'outputs', 'jailbreaks')\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    print(f\"Output will be saved to: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Load JailbreakBench Behaviors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading JailbreakBench behaviors...\")\n",
    "jbb = load_dataset(\"JailbreakBench/JBB-Behaviors\", \"behaviors\")\n",
    "\n",
    "behaviors = [\n",
    "    {\"id\": i, \"goal\": b[\"Goal\"], \"target\": b[\"Target\"]} \n",
    "    for i, b in enumerate(jbb[\"train\"])\n",
    "]\n",
    "\n",
    "print(f\"Loaded {len(behaviors)} harmful behaviors\")\n",
    "print(f\"Will generate jailbreaks for {min(N_PROMPTS, len(behaviors))} behaviors\")\n",
    "\n",
    "# Show examples\n",
    "print(\"\\nExample behaviors:\")\n",
    "for b in behaviors[:3]:\n",
    "    print(f\"  - {b['goal'][:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Load Model for GCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "print(\"This may take a few minutes...\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "print(f\"Model loaded!\")\n",
    "print(f\"Model device: {model.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Generate Jailbreaks with nanoGCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nanogcg\n",
    "\n",
    "# Configure GCG\n",
    "config = nanogcg.GCGConfig(\n",
    "    num_steps=GCG_NUM_STEPS,\n",
    "    search_width=GCG_SEARCH_WIDTH,\n",
    "    topk=GCG_TOPK,\n",
    "    seed=GCG_SEED\n",
    ")\n",
    "\n",
    "print(f\"GCG Config:\")\n",
    "print(f\"  num_steps: {GCG_NUM_STEPS}\")\n",
    "print(f\"  search_width: {GCG_SEARCH_WIDTH}\")\n",
    "print(f\"  topk: {GCG_TOPK}\")\n",
    "print(f\"  seed: {GCG_SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate jailbreaks\n",
    "print(f\"\\nGenerating {N_PROMPTS} jailbreaks...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "jailbreaks = []\n",
    "failed = []\n",
    "\n",
    "for i, behavior in enumerate(tqdm(behaviors[:N_PROMPTS], desc=\"Generating jailbreaks\")):\n",
    "    try:\n",
    "        result = nanogcg.run(\n",
    "            model, \n",
    "            tokenizer, \n",
    "            behavior[\"goal\"], \n",
    "            behavior[\"target\"], \n",
    "            config\n",
    "        )\n",
    "        \n",
    "        jailbreaks.append({\n",
    "            \"id\": behavior[\"id\"],\n",
    "            \"goal\": behavior[\"goal\"],\n",
    "            \"target\": behavior[\"target\"],\n",
    "            \"suffix\": result.best_string,\n",
    "            \"loss\": float(result.best_loss),\n",
    "            \"jailbreak_prompt\": f\"{behavior['goal']} {result.best_string}\"\n",
    "        })\n",
    "        print(f\"  [{i+1}/{N_PROMPTS}] Success! Loss: {result.best_loss:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  [{i+1}/{N_PROMPTS}] Failed: {e}\")\n",
    "        failed.append({\"id\": behavior[\"id\"], \"goal\": behavior[\"goal\"], \"error\": str(e)})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"Completed: {len(jailbreaks)} successful, {len(failed)} failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "df = pd.DataFrame(jailbreaks)\n",
    "\n",
    "# Display summary\n",
    "print(\"Generated Jailbreaks Summary:\")\n",
    "print(f\"  Total: {len(df)}\")\n",
    "print(f\"  Average loss: {df['loss'].mean():.4f}\")\n",
    "print(f\"  Min loss: {df['loss'].min():.4f}\")\n",
    "print(f\"  Max loss: {df['loss'].max():.4f}\")\n",
    "\n",
    "# Show first few\n",
    "print(\"\\nFirst 3 jailbreaks:\")\n",
    "for _, row in df.head(3).iterrows():\n",
    "    print(f\"\\n  Goal: {row['goal'][:60]}...\")\n",
    "    print(f\"  Suffix: {row['suffix'][:60]}...\")\n",
    "    print(f\"  Loss: {row['loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "output_path = os.path.join(OUTPUT_DIR, OUTPUT_FILENAME)\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Saved {len(df)} jailbreaks to: {output_path}\")\n",
    "\n",
    "# Also save as .pt for downstream notebooks\n",
    "pt_path = os.path.join(OUTPUT_DIR, OUTPUT_FILENAME.replace('.csv', '.pt'))\n",
    "torch.save(jailbreaks, pt_path)\n",
    "print(f\"Saved torch format to: {pt_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display full DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free GPU memory\n",
    "del model\n",
    "del tokenizer\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Done! Model cleaned up.\")\n",
    "print(f\"\\nOutput saved to: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
