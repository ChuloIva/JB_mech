{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Attention Convergence Test Across Models\n",
    "\n",
    "Test whether different models attend to similar \"trigger tokens\" in harmful prompts.\n",
    "Uses HarmBench/JailbreakBench examples and multiple models to analyze attention patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Setup paths\n",
    "if os.path.basename(os.getcwd()) == 'notebooks':\n",
    "    PROJECT_ROOT = os.path.dirname(os.getcwd())\n",
    "else:\n",
    "    PROJECT_ROOT = os.getcwd()\n",
    "\n",
    "sys.path.insert(0, os.path.join(PROJECT_ROOT, 'src'))\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\nfrom collections import defaultdict\nfrom dataclasses import dataclass\nimport gc\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n\ntorch.set_grad_enabled(False)\n\nimport warnings\nwarnings.filterwarnings('ignore')"
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": "# Models to test\nMODELS_TO_TEST = {\n    \"gemma3-4b\": {\n        \"name\": \"google/gemma-3-4b-it\",\n        \"quantize\": False,\n    },\n    \"qwen3-4b\": {\n        \"name\": \"Qwen/Qwen3-4B\",\n        \"quantize\": False,\n    },\n    \"llama-8b-bnb\": {\n        \"name\": \"unsloth/Llama-3.1-8B-Instruct-bnb-4bit\",  # Pre-quantized BNB model\n        \"quantize\": False,  # Already quantized\n    },\n}\n\n# Experiment parameters\nN_EXAMPLES = 30\nTOP_K_TOKENS = 10\nMAX_NEW_TOKENS = 100\nLAYER_PERCENT = 50  # Use middle layer for attention\n\nprint(f\"Testing {len(MODELS_TO_TEST)} models on {N_EXAMPLES} examples\")\nprint(f\"Models: {list(MODELS_TO_TEST.keys())}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Load HarmBench Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": "from datasets import load_dataset\n\nprint(\"Loading JailbreakBench...\")\njbb = load_dataset(\"JailbreakBench/JBB-Behaviors\", \"behaviors\")\nall_harmful = list(jbb[\"harmful\"])\n\n# Sample N_EXAMPLES\nbehaviors = all_harmful[:N_EXAMPLES]\n\nprint(f\"Loaded {len(behaviors)} behaviors (from {len(all_harmful)} total)\")\nprint(f\"\\nExample behaviors:\")\nfor i, b in enumerate(behaviors[:3]):\n    print(f\"  [{i}] {b['Goal'][:80]}...\")\n    print(f\"      Category: {b.get('Category', 'N/A')}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Attention Capture Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": "@dataclass\nclass AttentionResult:\n    \"\"\"Stores attention capture results.\"\"\"\n    attention_weights: torch.Tensor  # [num_heads, seq_len, seq_len]\n    prompt_tokens: list[str]\n    response_tokens: list[str]\n    prompt_end_idx: int\n    \n    def get_response_to_prompt_attention(self) -> torch.Tensor:\n        \"\"\"Get attention from response tokens to prompt tokens.\"\"\"\n        # [heads, response_len, prompt_len]\n        return self.attention_weights[:, self.prompt_end_idx:, :self.prompt_end_idx]\n    \n    def get_prompt_attention_scores(self, head: int | None = None) -> torch.Tensor:\n        \"\"\"Get aggregated attention scores for each prompt token.\"\"\"\n        attn = self.get_response_to_prompt_attention().float()  # Convert to float32\n        \n        if head is not None:\n            attn = attn[head:head+1]\n        \n        # Average over heads and response positions\n        scores = attn.mean(dim=(0, 1))  # [prompt_len]\n        return scores\n    \n    def get_top_attended_tokens(self, k: int = 10) -> list[tuple[int, str, float]]:\n        \"\"\"Get top-k attended prompt tokens.\"\"\"\n        scores = self.get_prompt_attention_scores()\n        top_indices = torch.argsort(scores, descending=True)[:k]\n        \n        results = []\n        for idx in top_indices:\n            idx = idx.item()\n            if idx < len(self.prompt_tokens):\n                results.append((idx, self.prompt_tokens[idx], scores[idx].item()))\n        return results"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": "def load_model(model_config: dict, device: str = \"cuda\"):\n    \"\"\"Load model and tokenizer.\"\"\"\n    model_name = model_config[\"name\"]\n    \n    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n    \n    load_kwargs = {\n        \"trust_remote_code\": True,\n        \"torch_dtype\": torch.bfloat16,\n        \"attn_implementation\": \"eager\",  # Need eager for attention weights\n    }\n    \n    if model_config.get(\"quantize\", False):\n        bnb_config = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_compute_dtype=torch.bfloat16,\n            bnb_4bit_use_double_quant=True,\n            bnb_4bit_quant_type=\"nf4\",\n        )\n        load_kwargs[\"quantization_config\"] = bnb_config\n        load_kwargs[\"device_map\"] = \"auto\"\n    else:\n        load_kwargs[\"device_map\"] = device\n    \n    # Special handling for Gemma 3 - load text-only model without vision tower\n    if \"gemma-3\" in model_name.lower():\n        from transformers import AutoConfig\n        from transformers.models.gemma3.configuration_gemma3 import Gemma3TextConfig\n        from transformers.models.gemma3.modeling_gemma3 import Gemma3ForCausalLM\n        \n        print(\"Loading Gemma 3 in text-only mode (no vision tower)...\")\n        \n        # Extract text config from multimodal config\n        multi_config = AutoConfig.from_pretrained(model_name)\n        text_cfg_dict = multi_config.text_config.to_dict()\n        text_cfg_dict[\"vocab_size\"] = 262208\n        if tokenizer.pad_token_id is not None:\n            text_cfg_dict[\"pad_token_id\"] = tokenizer.pad_token_id\n        text_cfg_dict[\"bos_token_id\"] = tokenizer.bos_token_id\n        text_cfg_dict[\"eos_token_id\"] = tokenizer.eos_token_id\n        \n        text_config = Gemma3TextConfig(**text_cfg_dict)\n        \n        # Remove device_map for Gemma3ForCausalLM loading\n        gemma_kwargs = {k: v for k, v in load_kwargs.items() if k != \"device_map\"}\n        \n        model = Gemma3ForCausalLM.from_pretrained(\n            model_name,\n            config=text_config,\n            device_map=\"auto\",\n            **gemma_kwargs\n        )\n    else:\n        model = AutoModelForCausalLM.from_pretrained(model_name, **load_kwargs)\n    \n    model.eval()\n    \n    return model, tokenizer\n\n\ndef get_num_layers(model) -> int:\n    \"\"\"Get number of layers in the model.\"\"\"\n    if hasattr(model, \"config\"):\n        if hasattr(model.config, \"num_hidden_layers\"):\n            return model.config.num_hidden_layers\n        if hasattr(model.config, \"n_layer\"):\n            return model.config.n_layer\n    return 32  # fallback\n\n\ndef cleanup_model(model, tokenizer):\n    \"\"\"Clean up model to free GPU memory.\"\"\"\n    del model\n    del tokenizer\n    gc.collect()\n    torch.cuda.empty_cache()\n    print(\"Model cleaned up\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture_attention(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt: str,\n",
    "    layer_idx: int,\n",
    "    max_new_tokens: int = 100,\n",
    ") -> AttentionResult:\n",
    "    \"\"\"Generate response and capture attention at specified layer.\"\"\"\n",
    "    \n",
    "    # Format prompt\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    formatted = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    \n",
    "    # Tokenize and find prompt boundary\n",
    "    prompt_inputs = tokenizer(formatted, return_tensors=\"pt\").to(model.device)\n",
    "    prompt_len = prompt_inputs[\"input_ids\"].shape[1]\n",
    "    \n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            prompt_inputs[\"input_ids\"],\n",
    "            attention_mask=prompt_inputs[\"attention_mask\"],\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "    \n",
    "    # Get full sequence\n",
    "    full_ids = output_ids[0]\n",
    "    \n",
    "    # Forward pass with attention output\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=full_ids.unsqueeze(0),\n",
    "            output_attentions=True,\n",
    "            return_dict=True,\n",
    "        )\n",
    "    \n",
    "    # Extract attention from target layer\n",
    "    # Shape: [batch, heads, seq_len, seq_len]\n",
    "    layer_attention = outputs.attentions[layer_idx][0]  # Remove batch dim\n",
    "    \n",
    "    # Decode tokens\n",
    "    all_tokens = [tokenizer.decode([tid]) for tid in full_ids]\n",
    "    prompt_tokens = all_tokens[:prompt_len]\n",
    "    response_tokens = all_tokens[prompt_len:]\n",
    "    \n",
    "    return AttentionResult(\n",
    "        attention_weights=layer_attention.cpu(),\n",
    "        prompt_tokens=prompt_tokens,\n",
    "        response_tokens=response_tokens,\n",
    "        prompt_end_idx=prompt_len,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Run Attention Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": "all_results = {}\n\nfor model_key, model_config in MODELS_TO_TEST.items():\n    print(f\"\\n{'='*60}\")\n    print(f\"Loading: {model_key} ({model_config['name']})\")\n    print(f\"{'='*60}\")\n    \n    model = None\n    tokenizer = None\n    \n    try:\n        model, tokenizer = load_model(model_config)\n        \n        num_layers = get_num_layers(model)\n        target_layer = int(num_layers * LAYER_PERCENT / 100)\n        print(f\"Layers: {num_layers}, using layer {target_layer}\")\n        \n        model_results = []\n        \n        for idx, behavior in enumerate(tqdm(behaviors, desc=f\"Processing {model_key}\")):\n            prompt = behavior[\"Goal\"]\n            \n            try:\n                result = capture_attention(\n                    model, tokenizer, prompt,\n                    layer_idx=target_layer,\n                    max_new_tokens=MAX_NEW_TOKENS,\n                )\n                \n                top_tokens = result.get_top_attended_tokens(k=TOP_K_TOKENS)\n                scores = result.get_prompt_attention_scores()\n                \n                model_results.append({\n                    \"behavior_id\": idx,\n                    \"category\": behavior.get(\"Category\", \"unknown\"),\n                    \"prompt\": prompt,\n                    \"prompt_tokens\": result.prompt_tokens,\n                    \"attention_scores\": scores.float().numpy(),\n                    \"top_tokens\": [(i, tok, float(score)) for i, tok, score in top_tokens],\n                    \"response_preview\": tokenizer.decode(\n                        tokenizer.encode(\"\".join(result.response_tokens))[:50],\n                        skip_special_tokens=True\n                    ),\n                })\n                \n            except Exception as e:\n                print(f\"\\nError on behavior {idx}: {e}\")\n                model_results.append({\n                    \"behavior_id\": idx,\n                    \"prompt\": prompt,\n                    \"error\": str(e),\n                })\n        \n        all_results[model_key] = model_results\n        print(f\"\\nCompleted {len(model_results)} examples\")\n        \n    except Exception as e:\n        print(f\"Failed to load {model_key}: {e}\")\n        all_results[model_key] = [{\"error\": str(e)}]\n    \n    finally:\n        # Aggressive cleanup\n        if model is not None:\n            del model\n        if tokenizer is not None:\n            del tokenizer\n        gc.collect()\n        torch.cuda.empty_cache()\n        torch.cuda.synchronize()\n        gc.collect()\n        print(f\"Cleaned up {model_key}\")\n\nprint(f\"\\n{'='*60}\")\nprint(\"Done!\")\nprint(f\"{'='*60}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Analyze Token Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_top_token_strings(results: list, top_k: int = 5) -> list:\n",
    "    \"\"\"Extract token strings from top attended tokens.\"\"\"\n",
    "    token_sets = []\n",
    "    for r in results:\n",
    "        if \"top_tokens\" in r:\n",
    "            tokens = [tok.strip().lower() for _, tok, _ in r[\"top_tokens\"][:top_k]]\n",
    "            token_sets.append(set(tokens))\n",
    "    return token_sets\n",
    "\n",
    "\n",
    "def compute_token_overlap(token_sets_a: list, token_sets_b: list) -> float:\n",
    "    \"\"\"Compute average Jaccard similarity.\"\"\"\n",
    "    similarities = []\n",
    "    for set_a, set_b in zip(token_sets_a, token_sets_b):\n",
    "        if len(set_a | set_b) > 0:\n",
    "            jaccard = len(set_a & set_b) / len(set_a | set_b)\n",
    "            similarities.append(jaccard)\n",
    "    return np.mean(similarities) if similarities else 0.0\n",
    "\n",
    "\n",
    "model_names = list(all_results.keys())\n",
    "n_models = len(model_names)\n",
    "\n",
    "print(\"Token Overlap Matrix (Jaccard Similarity of Top-5 Attended Tokens)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "overlap_matrix = np.zeros((n_models, n_models))\n",
    "\n",
    "for i, model_a in enumerate(model_names):\n",
    "    tokens_a = extract_top_token_strings(all_results[model_a], top_k=5)\n",
    "    for j, model_b in enumerate(model_names):\n",
    "        tokens_b = extract_top_token_strings(all_results[model_b], top_k=5)\n",
    "        overlap = compute_token_overlap(tokens_a, tokens_b)\n",
    "        overlap_matrix[i, j] = overlap\n",
    "\n",
    "overlap_df = pd.DataFrame(overlap_matrix, index=model_names, columns=model_names)\n",
    "print(overlap_df.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global token frequency\n",
    "print(\"\\nMost Commonly Attended Tokens Across All Models\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "global_token_counts = defaultdict(lambda: {\"count\": 0, \"total_score\": 0.0, \"models\": set()})\n",
    "\n",
    "for model_name, results in all_results.items():\n",
    "    for r in results:\n",
    "        if \"top_tokens\" in r:\n",
    "            for idx, tok, score in r[\"top_tokens\"][:5]:\n",
    "                tok_clean = tok.strip().lower()\n",
    "                if len(tok_clean) > 1:\n",
    "                    global_token_counts[tok_clean][\"count\"] += 1\n",
    "                    global_token_counts[tok_clean][\"total_score\"] += score\n",
    "                    global_token_counts[tok_clean][\"models\"].add(model_name)\n",
    "\n",
    "sorted_tokens = sorted(\n",
    "    global_token_counts.items(),\n",
    "    key=lambda x: (len(x[1][\"models\"]), x[1][\"count\"]),\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "print(f\"{'Token':<20} {'Count':<8} {'Avg Score':<12} {'Models'}\")\n",
    "print(\"-\"*60)\n",
    "for tok, stats in sorted_tokens[:20]:\n",
    "    avg_score = stats[\"total_score\"] / stats[\"count\"]\n",
    "    models_str = \", \".join(sorted(stats[\"models\"]))\n",
    "    print(f\"{tok:<20} {stats['count']:<8} {avg_score:<12.4f} {models_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-example comparison\n",
    "print(\"\\nPer-Example Token Agreement\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for example_idx in range(min(5, N_EXAMPLES)):\n",
    "    print(f\"\\n--- Example {example_idx} ---\")\n",
    "    \n",
    "    first_result = all_results[model_names[0]][example_idx]\n",
    "    if \"prompt\" in first_result:\n",
    "        print(f\"Prompt: {first_result['prompt'][:80]}...\")\n",
    "    \n",
    "    for model_name in model_names:\n",
    "        result = all_results[model_name][example_idx]\n",
    "        \n",
    "        if \"top_tokens\" in result:\n",
    "            top_3 = [f\"'{tok}'({score:.3f})\" for _, tok, score in result[\"top_tokens\"][:3]]\n",
    "            print(f\"  {model_name:<15}: {', '.join(top_3)}\")\n",
    "        else:\n",
    "            print(f\"  {model_name:<15}: [error]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "im = ax.imshow(overlap_matrix, cmap='Blues', vmin=0, vmax=1)\n",
    "\n",
    "ax.set_xticks(range(n_models))\n",
    "ax.set_yticks(range(n_models))\n",
    "ax.set_xticklabels(model_names, rotation=45, ha='right')\n",
    "ax.set_yticklabels(model_names)\n",
    "\n",
    "for i in range(n_models):\n",
    "    for j in range(n_models):\n",
    "        ax.text(j, i, f\"{overlap_matrix[i,j]:.2f}\", ha='center', va='center', fontsize=12)\n",
    "\n",
    "ax.set_title(\"Token Overlap (Jaccard Similarity)\\nTop-5 Attended Tokens\")\n",
    "plt.colorbar(im, ax=ax, label='Jaccard Similarity')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention score distributions\n",
    "fig, axes = plt.subplots(1, len(model_names), figsize=(4*len(model_names), 4), sharey=True)\n",
    "if len(model_names) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, model_name in zip(axes, model_names):\n",
    "    scores = []\n",
    "    for r in all_results[model_name]:\n",
    "        if \"top_tokens\" in r:\n",
    "            scores.extend([score for _, _, score in r[\"top_tokens\"][:5]])\n",
    "    \n",
    "    if scores:\n",
    "        ax.hist(scores, bins=20, alpha=0.7, edgecolor='black')\n",
    "        ax.set_xlim(0, max(scores) * 1.1)\n",
    "    ax.set_xlabel('Attention Score')\n",
    "    ax.set_title(model_name)\n",
    "\n",
    "axes[0].set_ylabel('Frequency')\n",
    "fig.suptitle('Distribution of Top-5 Attention Scores', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_serializable(obj):\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: make_serializable(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [make_serializable(v) for v in obj]\n",
    "    elif isinstance(obj, set):\n",
    "        return list(obj)\n",
    "    return obj\n",
    "\n",
    "output_dir = os.path.join(PROJECT_ROOT, \"results\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = os.path.join(output_dir, f\"attention_convergence_{timestamp}.json\")\n",
    "\n",
    "save_data = {\n",
    "    \"config\": {\n",
    "        \"n_examples\": N_EXAMPLES,\n",
    "        \"top_k_tokens\": TOP_K_TOKENS,\n",
    "        \"models\": {k: v[\"name\"] for k, v in MODELS_TO_TEST.items()},\n",
    "        \"layer_percent\": LAYER_PERCENT,\n",
    "    },\n",
    "    \"overlap_matrix\": overlap_matrix.tolist(),\n",
    "    \"model_names\": model_names,\n",
    "    \"results\": make_serializable(all_results),\n",
    "}\n",
    "\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(save_data, f, indent=2)\n",
    "\n",
    "print(f\"Saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "mask = ~np.eye(n_models, dtype=bool)\n",
    "avg_overlap = overlap_matrix[mask].mean() if n_models > 1 else 0.0\n",
    "print(f\"\\nAverage cross-model token overlap: {avg_overlap:.3f}\")\n",
    "\n",
    "if n_models > 1:\n",
    "    upper_tri = np.triu_indices(n_models, k=1)\n",
    "    overlaps = overlap_matrix[upper_tri]\n",
    "    \n",
    "    max_idx = np.argmax(overlaps)\n",
    "    min_idx = np.argmin(overlaps)\n",
    "    \n",
    "    print(f\"Most similar: {model_names[upper_tri[0][max_idx]]} <-> {model_names[upper_tri[1][max_idx]]} ({overlaps[max_idx]:.3f})\")\n",
    "    print(f\"Least similar: {model_names[upper_tri[0][min_idx]]} <-> {model_names[upper_tri[1][min_idx]]} ({overlaps[min_idx]:.3f})\")\n",
    "\n",
    "print(\"\\nPer-model stats:\")\n",
    "for model_name in model_names:\n",
    "    results = all_results[model_name]\n",
    "    valid = [r for r in results if \"top_tokens\" in r]\n",
    "    \n",
    "    if valid:\n",
    "        all_scores = [score for r in valid for _, _, score in r[\"top_tokens\"][:5]]\n",
    "        print(f\"  {model_name}: {len(valid)}/{len(results)} valid, avg attn={np.mean(all_scores):.4f}, max={np.max(all_scores):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}