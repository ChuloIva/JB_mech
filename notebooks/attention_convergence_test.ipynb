{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Attention Convergence Test Across Models (Colab Version)\n",
    "\n",
    "Test whether different model families and sizes attend to similar \"trigger tokens\" in harmful prompts.\n",
    "\n",
    "**Efficient approach**: Single forward pass, extract last-token-to-all attention (no generation needed).\n",
    "\n",
    "**Models tested**: Llama, Qwen, Gemma, Phi families at multiple sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab Setup - Run this first!\n",
    "# Mount Google Drive for saving results\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Install dependencies\n",
    "!pip install -q transformers accelerate bitsandbytes datasets huggingface_hub\n",
    "\n",
    "# Login to HuggingFace (needed for Llama, Gemma)\n",
    "from huggingface_hub import login\n",
    "login()  # Enter your HF token when prompted\n",
    "\n",
    "import os\n",
    "RESULTS_DIR = \"/content/drive/MyDrive/attention_convergence_results\"\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "print(f\"Results will be saved to: {RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "import json\n",
    "import gc\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models to test - organized by family and size\n",
    "MODELS_TO_TEST = {\n",
    "    # Llama family (Meta) - 4 sizes\n",
    "    \"llama-1b\": \"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "    \"llama-3b\": \"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    \"llama-8b\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    # \"llama-70b\": \"meta-llama/Llama-3.1-70B-Instruct\",\n",
    "    \n",
    "    # Qwen family (Alibaba) - 5 sizes\n",
    "    # \"qwen-0.5b\": \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
    "    \"qwen-3b\": \"Qwen/Qwen2.5-3B-Instruct\",\n",
    "    \"qwen-7b\": \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "    \"qwen-14b\": \"Qwen/Qwen2.5-14B-Instruct\",\n",
    "    # \"qwen-72b\": \"Qwen/Qwen2.5-72B-Instruct\",\n",
    "    \n",
    "    # Gemma family (Google) - 3 sizes\n",
    "    \"gemma-2b\": \"google/gemma-2-2b-it\",\n",
    "    \"gemma-9b\": \"google/gemma-2-9b-it\",\n",
    "    \"gemma-27b\": \"google/gemma-2-27b-it\",\n",
    "    \n",
    "    # Phi family (Microsoft) - 3 sizes\n",
    "    # \"phi-mini\": \"microsoft/Phi-3-mini-4k-instruct\",      # 3.8B\n",
    "    # \"phi-small\": \"microsoft/Phi-3-small-8k-instruct\",   # 7B\n",
    "    # \"phi-medium\": \"microsoft/Phi-3-medium-4k-instruct\", # 14B\n",
    "}\n",
    "\n",
    "# Experiment parameters\n",
    "N_EXAMPLES = 50  # Number of harmful prompts to test\n",
    "TOP_K_TOKENS = 10  # Top attended tokens to track\n",
    "LAYER_PERCENT = 50  # Use middle layer (50% through)\n",
    "\n",
    "# Quantization settings (for memory efficiency)\n",
    "QUANTIZE_THRESHOLD_B = 8  # Quantize models >= 8B parameters\n",
    "\n",
    "print(f\"Testing {len(MODELS_TO_TEST)} models on {N_EXAMPLES} examples\")\n",
    "print(f\"\\nModels by family:\")\n",
    "for family in [\"llama\", \"qwen\", \"gemma\", \"phi\"]:\n",
    "    models = [k for k in MODELS_TO_TEST.keys() if k.startswith(family)]\n",
    "    print(f\"  {family.capitalize()}: {models}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Load HarmBench Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading JailbreakBench behaviors...\")\n",
    "jbb = load_dataset(\"JailbreakBench/JBB-Behaviors\", \"behaviors\")\n",
    "all_harmful = list(jbb[\"harmful\"])\n",
    "\n",
    "# Sample N_EXAMPLES\n",
    "behaviors = all_harmful[:N_EXAMPLES]\n",
    "\n",
    "print(f\"Loaded {len(behaviors)} behaviors (from {len(all_harmful)} total)\")\n",
    "print(f\"\\nExample behaviors:\")\n",
    "for i, b in enumerate(behaviors[:3]):\n",
    "    print(f\"  [{i}] {b['Goal'][:80]}...\")\n",
    "    print(f\"      Category: {b.get('Category', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Attention Capture Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class AttentionResult:\n",
    "    \"\"\"Stores attention capture results for a single prompt.\"\"\"\n",
    "    attention_weights: torch.Tensor  # [num_heads, prompt_len] - last token attending to all\n",
    "    tokens: list[str]  # Decoded prompt tokens\n",
    "    token_ids: list[int]  # Raw token IDs\n",
    "    \n",
    "    def get_attention_scores(self, head: int | None = None) -> torch.Tensor:\n",
    "        \"\"\"Get attention scores for each prompt token.\n",
    "        \n",
    "        Args:\n",
    "            head: Specific head index, or None for average across all heads.\n",
    "        \"\"\"\n",
    "        attn = self.attention_weights.float()  # [heads, prompt_len]\n",
    "        \n",
    "        if head is not None:\n",
    "            return attn[head]  # [prompt_len]\n",
    "        else:\n",
    "            return attn.mean(dim=0)  # Average over heads -> [prompt_len]\n",
    "    \n",
    "    def get_top_attended_tokens(self, k: int = 10) -> list[tuple[int, str, float]]:\n",
    "        \"\"\"Get top-k attended prompt tokens.\n",
    "        \n",
    "        Returns:\n",
    "            List of (position, token_string, attention_score) tuples.\n",
    "        \"\"\"\n",
    "        scores = self.get_attention_scores()\n",
    "        top_indices = torch.argsort(scores, descending=True)[:k]\n",
    "        \n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            idx = idx.item()\n",
    "            if idx < len(self.tokens):\n",
    "                results.append((idx, self.tokens[idx], scores[idx].item()))\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_model_size(model_name: str) -> float:\n",
    "    \"\"\"Estimate model size in billions from name.\"\"\"\n",
    "    import re\n",
    "    # Look for patterns like \"70B\", \"7b\", \"0.5B\", etc.\n",
    "    match = re.search(r'(\\d+\\.?\\d*)b', model_name.lower())\n",
    "    if match:\n",
    "        return float(match.group(1))\n",
    "    # Special cases\n",
    "    if \"mini\" in model_name.lower():\n",
    "        return 3.8\n",
    "    if \"small\" in model_name.lower():\n",
    "        return 7.0\n",
    "    if \"medium\" in model_name.lower():\n",
    "        return 14.0\n",
    "    return 7.0  # Default assumption\n",
    "\n",
    "\n",
    "def load_model(model_name: str, quantize_threshold: float = 8.0):\n",
    "    \"\"\"Load model and tokenizer with appropriate settings.\n",
    "    \n",
    "    Args:\n",
    "        model_name: HuggingFace model identifier.\n",
    "        quantize_threshold: Quantize models >= this size (in B params).\n",
    "    \"\"\"\n",
    "    estimated_size = estimate_model_size(model_name)\n",
    "    should_quantize = estimated_size >= quantize_threshold\n",
    "    \n",
    "    print(f\"  Estimated size: {estimated_size}B, quantize: {should_quantize}\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    load_kwargs = {\n",
    "        \"trust_remote_code\": True,\n",
    "        \"torch_dtype\": torch.bfloat16,\n",
    "        \"attn_implementation\": \"eager\",  # Required for output_attentions\n",
    "        \"device_map\": \"auto\",\n",
    "    }\n",
    "    \n",
    "    if should_quantize:\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "        )\n",
    "        load_kwargs[\"quantization_config\"] = bnb_config\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, **load_kwargs)\n",
    "    model.eval()\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def get_num_layers(model) -> int:\n",
    "    \"\"\"Get number of layers in the model.\"\"\"\n",
    "    if hasattr(model, \"config\"):\n",
    "        if hasattr(model.config, \"num_hidden_layers\"):\n",
    "            return model.config.num_hidden_layers\n",
    "        if hasattr(model.config, \"n_layer\"):\n",
    "            return model.config.n_layer\n",
    "    return 32  # fallback\n",
    "\n",
    "\n",
    "def cleanup_model(model, tokenizer):\n",
    "    \"\"\"Clean up model to free GPU memory.\"\"\"\n",
    "    del model\n",
    "    del tokenizer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture_attention(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt: str,\n",
    "    layer_idx: int,\n",
    ") -> AttentionResult:\n",
    "    \"\"\"Capture last-token-to-all attention with a single forward pass.\n",
    "    \n",
    "    This is much faster than generating a response - we only need to\n",
    "    process the prompt once to see what the model attends to.\n",
    "    \n",
    "    Args:\n",
    "        model: The language model.\n",
    "        tokenizer: The tokenizer.\n",
    "        prompt: The user prompt text.\n",
    "        layer_idx: Which layer to extract attention from.\n",
    "    \n",
    "    Returns:\n",
    "        AttentionResult with attention weights and token info.\n",
    "    \"\"\"\n",
    "    # Format prompt with chat template\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    formatted = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(formatted, return_tensors=\"pt\").to(model.device)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    seq_len = input_ids.shape[1]\n",
    "    \n",
    "    # Single forward pass with attention output\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            output_attentions=True,\n",
    "            return_dict=True,\n",
    "        )\n",
    "    \n",
    "    # Extract attention from target layer\n",
    "    # Shape: [batch, heads, seq_len, seq_len]\n",
    "    layer_attention = outputs.attentions[layer_idx][0]  # Remove batch dim\n",
    "    \n",
    "    # Get last token attending to all previous tokens\n",
    "    # Shape: [heads, seq_len-1] (excluding self-attention to last token)\n",
    "    last_token_attention = layer_attention[:, -1, :-1]\n",
    "    \n",
    "    # Decode tokens (excluding the last token which is usually generation prompt)\n",
    "    token_ids = input_ids[0, :-1].tolist()\n",
    "    tokens = [tokenizer.decode([tid]) for tid in token_ids]\n",
    "    \n",
    "    return AttentionResult(\n",
    "        attention_weights=last_token_attention.cpu(),\n",
    "        tokens=tokens,\n",
    "        token_ids=token_ids,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Run Attention Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = {}\n",
    "\n",
    "for model_key, model_name in MODELS_TO_TEST.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Loading: {model_key} ({model_name})\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    model = None\n",
    "    tokenizer = None\n",
    "    \n",
    "    try:\n",
    "        model, tokenizer = load_model(model_name, quantize_threshold=QUANTIZE_THRESHOLD_B)\n",
    "        \n",
    "        num_layers = get_num_layers(model)\n",
    "        target_layer = int(num_layers * LAYER_PERCENT / 100)\n",
    "        print(f\"  Layers: {num_layers}, using layer {target_layer}\")\n",
    "        \n",
    "        model_results = []\n",
    "        \n",
    "        for idx, behavior in enumerate(tqdm(behaviors, desc=f\"{model_key}\")):\n",
    "            prompt = behavior[\"Goal\"]\n",
    "            \n",
    "            try:\n",
    "                result = capture_attention(\n",
    "                    model, tokenizer, prompt,\n",
    "                    layer_idx=target_layer,\n",
    "                )\n",
    "                \n",
    "                top_tokens = result.get_top_attended_tokens(k=TOP_K_TOKENS)\n",
    "                scores = result.get_attention_scores()\n",
    "                \n",
    "                model_results.append({\n",
    "                    \"behavior_id\": idx,\n",
    "                    \"category\": behavior.get(\"Category\", \"unknown\"),\n",
    "                    \"prompt\": prompt,\n",
    "                    \"tokens\": result.tokens,\n",
    "                    \"attention_scores\": scores.numpy().tolist(),\n",
    "                    \"top_tokens\": [(i, tok, float(score)) for i, tok, score in top_tokens],\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\\n  Error on behavior {idx}: {e}\")\n",
    "                model_results.append({\n",
    "                    \"behavior_id\": idx,\n",
    "                    \"prompt\": prompt,\n",
    "                    \"error\": str(e),\n",
    "                })\n",
    "        \n",
    "        all_results[model_key] = model_results\n",
    "        print(f\"  Completed {len(model_results)} examples\")\n",
    "        \n",
    "        # Save intermediate results after each model (in case of crash)\n",
    "        intermediate_file = os.path.join(RESULTS_DIR, f\"intermediate_{model_key}.json\")\n",
    "        with open(intermediate_file, \"w\") as f:\n",
    "            json.dump({\"model\": model_key, \"results\": model_results}, f)\n",
    "        print(f\"  Saved intermediate: {intermediate_file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Failed to load {model_key}: {e}\")\n",
    "        all_results[model_key] = [{\"error\": str(e)}]\n",
    "    \n",
    "    finally:\n",
    "        if model is not None or tokenizer is not None:\n",
    "            cleanup_model(model, tokenizer)\n",
    "            print(f\"  Cleaned up {model_key}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Done processing all models!\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Analyze Token Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_top_token_strings(results: list, top_k: int = 5) -> list:\n",
    "    \"\"\"Extract normalized token strings from top attended tokens.\"\"\"\n",
    "    token_sets = []\n",
    "    for r in results:\n",
    "        if \"top_tokens\" in r:\n",
    "            # Normalize: lowercase, strip whitespace\n",
    "            tokens = [tok.strip().lower() for _, tok, _ in r[\"top_tokens\"][:top_k]]\n",
    "            token_sets.append(set(tokens))\n",
    "    return token_sets\n",
    "\n",
    "\n",
    "def compute_token_overlap(token_sets_a: list, token_sets_b: list) -> float:\n",
    "    \"\"\"Compute average Jaccard similarity between paired token sets.\"\"\"\n",
    "    similarities = []\n",
    "    for set_a, set_b in zip(token_sets_a, token_sets_b):\n",
    "        if len(set_a | set_b) > 0:\n",
    "            jaccard = len(set_a & set_b) / len(set_a | set_b)\n",
    "            similarities.append(jaccard)\n",
    "    return np.mean(similarities) if similarities else 0.0\n",
    "\n",
    "\n",
    "def get_model_family(model_key: str) -> str:\n",
    "    \"\"\"Extract family name from model key.\"\"\"\n",
    "    return model_key.split(\"-\")[0]\n",
    "\n",
    "\n",
    "# Build overlap matrix\n",
    "model_names = [k for k in all_results.keys() if \"error\" not in all_results[k][0]]\n",
    "n_models = len(model_names)\n",
    "\n",
    "print(\"Token Overlap Matrix (Jaccard Similarity of Top-5 Attended Tokens)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "overlap_matrix = np.zeros((n_models, n_models))\n",
    "\n",
    "for i, model_a in enumerate(model_names):\n",
    "    tokens_a = extract_top_token_strings(all_results[model_a], top_k=5)\n",
    "    for j, model_b in enumerate(model_names):\n",
    "        tokens_b = extract_top_token_strings(all_results[model_b], top_k=5)\n",
    "        overlap = compute_token_overlap(tokens_a, tokens_b)\n",
    "        overlap_matrix[i, j] = overlap\n",
    "\n",
    "overlap_df = pd.DataFrame(overlap_matrix, index=model_names, columns=model_names)\n",
    "print(overlap_df.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global token frequency analysis\n",
    "print(\"\\nMost Commonly Attended Tokens Across All Models\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "global_token_counts = defaultdict(lambda: {\"count\": 0, \"total_score\": 0.0, \"models\": set(), \"families\": set()})\n",
    "\n",
    "for model_name, results in all_results.items():\n",
    "    family = get_model_family(model_name)\n",
    "    for r in results:\n",
    "        if \"top_tokens\" in r:\n",
    "            for idx, tok, score in r[\"top_tokens\"][:5]:\n",
    "                tok_clean = tok.strip().lower()\n",
    "                if len(tok_clean) > 1:  # Skip single chars\n",
    "                    global_token_counts[tok_clean][\"count\"] += 1\n",
    "                    global_token_counts[tok_clean][\"total_score\"] += score\n",
    "                    global_token_counts[tok_clean][\"models\"].add(model_name)\n",
    "                    global_token_counts[tok_clean][\"families\"].add(family)\n",
    "\n",
    "# Sort by number of families, then count\n",
    "sorted_tokens = sorted(\n",
    "    global_token_counts.items(),\n",
    "    key=lambda x: (len(x[1][\"families\"]), len(x[1][\"models\"]), x[1][\"count\"]),\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "print(f\"{'Token':<20} {'Families':<10} {'Models':<8} {'Count':<8} {'Avg Score':<12}\")\n",
    "print(\"-\"*70)\n",
    "for tok, stats in sorted_tokens[:25]:\n",
    "    avg_score = stats[\"total_score\"] / stats[\"count\"]\n",
    "    n_families = len(stats[\"families\"])\n",
    "    n_models = len(stats[\"models\"])\n",
    "    print(f\"{tok:<20} {n_families:<10} {n_models:<8} {stats['count']:<8} {avg_score:<12.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-example token agreement across models\n",
    "print(\"\\nPer-Example Token Agreement (first 5 examples)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for example_idx in range(min(5, N_EXAMPLES)):\n",
    "    print(f\"\\n--- Example {example_idx} ---\")\n",
    "    \n",
    "    # Get prompt from first valid model\n",
    "    for mn in model_names:\n",
    "        if example_idx < len(all_results[mn]):\n",
    "            result = all_results[mn][example_idx]\n",
    "            if \"prompt\" in result:\n",
    "                print(f\"Prompt: {result['prompt'][:80]}...\")\n",
    "                break\n",
    "    \n",
    "    # Show top tokens for each model\n",
    "    for model_name in model_names:\n",
    "        if example_idx < len(all_results[model_name]):\n",
    "            result = all_results[model_name][example_idx]\n",
    "            \n",
    "            if \"top_tokens\" in result:\n",
    "                top_3 = [f\"'{tok}'({score:.3f})\" for _, tok, score in result[\"top_tokens\"][:3]]\n",
    "                print(f\"  {model_name:<15}: {', '.join(top_3)}\")\n",
    "            else:\n",
    "                print(f\"  {model_name:<15}: [error]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create color mapping for families\n",
    "family_colors = {\"llama\": \"C0\", \"qwen\": \"C1\", \"gemma\": \"C2\", \"phi\": \"C3\"}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "im = ax.imshow(overlap_matrix, cmap='Blues', vmin=0, vmax=1)\n",
    "\n",
    "ax.set_xticks(range(n_models))\n",
    "ax.set_yticks(range(n_models))\n",
    "ax.set_xticklabels(model_names, rotation=45, ha='right', fontsize=9)\n",
    "ax.set_yticklabels(model_names, fontsize=9)\n",
    "\n",
    "# Add values to cells\n",
    "for i in range(n_models):\n",
    "    for j in range(n_models):\n",
    "        val = overlap_matrix[i, j]\n",
    "        color = 'white' if val > 0.5 else 'black'\n",
    "        ax.text(j, i, f\"{val:.2f}\", ha='center', va='center', fontsize=8, color=color)\n",
    "\n",
    "ax.set_title(\"Token Overlap (Jaccard Similarity)\\nTop-5 Attended Tokens, Last-Token-to-All Attention\", fontsize=12)\n",
    "plt.colorbar(im, ax=ax, label='Jaccard Similarity', shrink=0.8)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save to Drive\n",
    "fig.savefig(os.path.join(RESULTS_DIR, \"overlap_matrix.png\"), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved: {RESULTS_DIR}/overlap_matrix.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size-based analysis: Does attention pattern change with model size?\n",
    "print(\"\\nSize Scaling Analysis\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Extract size from model key\n",
    "def get_model_size(model_key: str) -> float:\n",
    "    \"\"\"Extract model size in billions from key.\"\"\"\n",
    "    import re\n",
    "    match = re.search(r'(\\d+\\.?\\d*)b', model_key.lower())\n",
    "    if match:\n",
    "        return float(match.group(1))\n",
    "    if \"mini\" in model_key:\n",
    "        return 3.8\n",
    "    if \"small\" in model_key:\n",
    "        return 7.0\n",
    "    if \"medium\" in model_key:\n",
    "        return 14.0\n",
    "    return 0.0\n",
    "\n",
    "# Compute size correlation within families\n",
    "print(\"\\nWithin-family size correlation:\")\n",
    "for family in sorted(set(get_model_family(m) for m in model_names)):\n",
    "    family_models = sorted(\n",
    "        [m for m in model_names if get_model_family(m) == family],\n",
    "        key=get_model_size\n",
    "    )\n",
    "    \n",
    "    if len(family_models) < 2:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n  {family.upper()}:\")\n",
    "    for i, m1 in enumerate(family_models):\n",
    "        for m2 in family_models[i+1:]:\n",
    "            idx1, idx2 = model_names.index(m1), model_names.index(m2)\n",
    "            overlap = overlap_matrix[idx1, idx2]\n",
    "            size1, size2 = get_model_size(m1), get_model_size(m2)\n",
    "            print(f\"    {m1} ({size1}B) <-> {m2} ({size2}B): {overlap:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdnmsuz1mo5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Within-family vs cross-family comparison\n",
    "print(\"\\nWithin-Family vs Cross-Family Overlap\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "within_family = []\n",
    "cross_family = []\n",
    "\n",
    "for i, model_a in enumerate(model_names):\n",
    "    family_a = get_model_family(model_a)\n",
    "    for j, model_b in enumerate(model_names):\n",
    "        if i >= j:\n",
    "            continue\n",
    "        family_b = get_model_family(model_b)\n",
    "        \n",
    "        if family_a == family_b:\n",
    "            within_family.append(overlap_matrix[i, j])\n",
    "        else:\n",
    "            cross_family.append(overlap_matrix[i, j])\n",
    "\n",
    "print(f\"Within-family overlap:  mean={np.mean(within_family):.3f}, std={np.std(within_family):.3f}, n={len(within_family)}\")\n",
    "print(f\"Cross-family overlap:   mean={np.mean(cross_family):.3f}, std={np.std(cross_family):.3f}, n={len(cross_family)}\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Box plot comparison\n",
    "axes[0].boxplot([within_family, cross_family], labels=['Within Family', 'Cross Family'])\n",
    "axes[0].set_ylabel('Jaccard Similarity')\n",
    "axes[0].set_title('Token Overlap: Within vs Cross Family')\n",
    "\n",
    "# Per-family average overlap\n",
    "families = list(set(get_model_family(m) for m in model_names))\n",
    "family_avg_overlap = {}\n",
    "\n",
    "for family in families:\n",
    "    family_models = [m for m in model_names if get_model_family(m) == family]\n",
    "    if len(family_models) > 1:\n",
    "        indices = [model_names.index(m) for m in family_models]\n",
    "        overlaps = []\n",
    "        for i in indices:\n",
    "            for j in indices:\n",
    "                if i < j:\n",
    "                    overlaps.append(overlap_matrix[i, j])\n",
    "        family_avg_overlap[family] = np.mean(overlaps) if overlaps else 0\n",
    "\n",
    "family_colors = {\"llama\": \"C0\", \"qwen\": \"C1\", \"gemma\": \"C2\", \"phi\": \"C3\"}\n",
    "axes[1].bar(family_avg_overlap.keys(), family_avg_overlap.values(), color=[family_colors.get(f, 'gray') for f in family_avg_overlap.keys()])\n",
    "axes[1].set_ylabel('Avg Jaccard Similarity')\n",
    "axes[1].set_title('Within-Family Average Overlap')\n",
    "axes[1].set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig(os.path.join(RESULTS_DIR, \"family_comparison.png\"), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved: {RESULTS_DIR}/family_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final results to Google Drive\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = os.path.join(RESULTS_DIR, f\"attention_convergence_{timestamp}.json\")\n",
    "\n",
    "# Prepare serializable data\n",
    "def make_serializable(obj):\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: make_serializable(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [make_serializable(v) for v in obj]\n",
    "    elif isinstance(obj, set):\n",
    "        return list(obj)\n",
    "    elif isinstance(obj, (np.float32, np.float64)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, (np.int32, np.int64)):\n",
    "        return int(obj)\n",
    "    return obj\n",
    "\n",
    "save_data = {\n",
    "    \"config\": {\n",
    "        \"n_examples\": N_EXAMPLES,\n",
    "        \"top_k_tokens\": TOP_K_TOKENS,\n",
    "        \"layer_percent\": LAYER_PERCENT,\n",
    "        \"quantize_threshold_b\": QUANTIZE_THRESHOLD_B,\n",
    "        \"models\": dict(MODELS_TO_TEST),\n",
    "        \"attention_method\": \"last_token_to_all\",\n",
    "    },\n",
    "    \"overlap_matrix\": overlap_matrix.tolist(),\n",
    "    \"model_names\": model_names,\n",
    "    \"within_family_overlap\": {\n",
    "        \"mean\": float(np.mean(within_family)),\n",
    "        \"std\": float(np.std(within_family)),\n",
    "        \"values\": within_family,\n",
    "    },\n",
    "    \"cross_family_overlap\": {\n",
    "        \"mean\": float(np.mean(cross_family)),\n",
    "        \"std\": float(np.std(cross_family)),\n",
    "        \"values\": cross_family,\n",
    "    },\n",
    "    \"results\": make_serializable(all_results),\n",
    "}\n",
    "\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(save_data, f, indent=2)\n",
    "\n",
    "print(f\"Saved final results: {output_file}\")\n",
    "print(f\"File size: {os.path.getsize(output_file) / 1e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Overall statistics\n",
    "mask = ~np.eye(n_models, dtype=bool)\n",
    "avg_overlap = overlap_matrix[mask].mean() if n_models > 1 else 0.0\n",
    "print(f\"\\nOverall average token overlap: {avg_overlap:.3f}\")\n",
    "\n",
    "# Within vs cross family\n",
    "print(f\"\\nWithin-family avg:  {np.mean(within_family):.3f}\")\n",
    "print(f\"Cross-family avg:   {np.mean(cross_family):.3f}\")\n",
    "print(f\"Difference:         {np.mean(within_family) - np.mean(cross_family):.3f}\")\n",
    "\n",
    "# Most/least similar pairs\n",
    "if n_models > 1:\n",
    "    upper_tri = np.triu_indices(n_models, k=1)\n",
    "    overlaps = overlap_matrix[upper_tri]\n",
    "    \n",
    "    max_idx = np.argmax(overlaps)\n",
    "    min_idx = np.argmin(overlaps)\n",
    "    \n",
    "    print(f\"\\nMost similar pair:  {model_names[upper_tri[0][max_idx]]} <-> {model_names[upper_tri[1][max_idx]]} ({overlaps[max_idx]:.3f})\")\n",
    "    print(f\"Least similar pair: {model_names[upper_tri[0][min_idx]]} <-> {model_names[upper_tri[1][min_idx]]} ({overlaps[min_idx]:.3f})\")\n",
    "\n",
    "# Per-family summary\n",
    "print(\"\\nPer-family statistics:\")\n",
    "for family in sorted(families):\n",
    "    family_models = [m for m in model_names if get_model_family(m) == family]\n",
    "    valid_results = sum(1 for m in family_models for r in all_results[m] if \"top_tokens\" in r)\n",
    "    total_results = sum(len(all_results[m]) for m in family_models)\n",
    "    print(f\"  {family:<8}: {len(family_models)} models, {valid_results}/{total_results} valid examples\")\n",
    "\n",
    "print(f\"\\nResults saved to: {RESULTS_DIR}\")\n",
    "print(\"Files:\")\n",
    "print(f\"  - attention_convergence_{timestamp}.json (full data)\")\n",
    "print(f\"  - overlap_matrix.png\")\n",
    "print(f\"  - family_comparison.png\")\n",
    "print(f\"  - intermediate_*.json (per-model backups)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
