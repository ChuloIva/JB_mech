{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Assistant Axis for Llama 3.1 8B\n",
    "\n",
    "This notebook computes the Assistant Axis for Llama 3.1 8B.\n",
    "\n",
    "**Storage-optimized:** Only saves the final axis and lightweight role vectors. Heavy intermediate data (responses, activations) is processed in memory and discarded.\n",
    "\n",
    "**Saved outputs (~150MB total):**\n",
    "- `axis.pt` - The final Assistant Axis (~500KB)\n",
    "- `role_vectors.pt` - Per-role mean vectors (~140MB)\n",
    "\n",
    "**NOT saved (processed in memory):**\n",
    "- Raw responses (~GB)\n",
    "- Full activations (~100GB+)\n",
    "\n",
    "**Runtime:** ~4-8 hours on A100\n",
    "\n",
    "**Requirements:**\n",
    "- GPU: A100 40GB recommended\n",
    "- OpenAI API key for judging responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "print(f\"Running in Colab: {IN_COLAB}\")\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Output directory in Drive (only for final outputs)\n",
    "    OUTPUT_DIR = '/content/drive/MyDrive/JB_mech_outputs/llama-3.1-8b'\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    print(f\"Final outputs will be saved to: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repos\n",
    "if IN_COLAB:\n",
    "    # Clone assistant-axis for role data and utilities\n",
    "    !git clone --depth 1 https://github.com/safety-research/assistant-axis.git /content/assistant-axis\n",
    "    ASSISTANT_AXIS_DIR = '/content/assistant-axis'\n",
    "else:\n",
    "    PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath('__file__')))\n",
    "    ASSISTANT_AXIS_DIR = os.path.join(PROJECT_ROOT, 'third_party', 'assistant-axis')\n",
    "    OUTPUT_DIR = os.path.join(PROJECT_ROOT, 'outputs', 'llama-3.1-8b')\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "if IN_COLAB:\n",
    "    !pip install -q torch transformers accelerate vllm\n",
    "    !pip install -q openai python-dotenv jsonlines tqdm\n",
    "    print(\"Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "TARGET_LAYER = 16\n",
    "TOTAL_LAYERS = 32\n",
    "HIDDEN_SIZE = 4096\n",
    "\n",
    "# Paths\n",
    "AXIS_PATH = os.path.join(OUTPUT_DIR, 'axis.pt')\n",
    "ROLE_VECTORS_PATH = os.path.join(OUTPUT_DIR, 'role_vectors.pt')\n",
    "\n",
    "# Pipeline settings (reduce for faster testing)\n",
    "N_QUESTIONS = 50  # Full: 240, Quick test: 20\n",
    "N_SYSTEM_PROMPTS = 3  # Full: 5, Quick: 2\n",
    "\n",
    "print(f\"Output: {OUTPUT_DIR}\")\n",
    "print(f\"Settings: {N_QUESTIONS} questions × {N_SYSTEM_PROMPTS} system prompts per role\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set OpenAI API key (paste your key here or use environment variable)\n",
    "OPENAI_API_KEY = \"\"  # <-- Paste your key here if not using env var\n",
    "\n",
    "if OPENAI_API_KEY:\n",
    "    os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
    "elif 'OPENAI_API_KEY' not in os.environ:\n",
    "    from getpass import getpass\n",
    "    api_key = getpass(\"Enter OpenAI API key: \")\n",
    "    os.environ['OPENAI_API_KEY'] = api_key\n",
    "\n",
    "print(\"OpenAI API key configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: Load Role Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "\n",
    "# Load roles\n",
    "ROLES_DIR = Path(ASSISTANT_AXIS_DIR) / 'data' / 'roles' / 'instructions'\n",
    "QUESTIONS_FILE = Path(ASSISTANT_AXIS_DIR) / 'data' / 'extraction_questions.jsonl'\n",
    "\n",
    "roles = {}\n",
    "for role_file in ROLES_DIR.glob('*.json'):\n",
    "    with open(role_file) as f:\n",
    "        roles[role_file.stem] = json.load(f)\n",
    "\n",
    "# Load questions\n",
    "with jsonlines.open(QUESTIONS_FILE) as reader:\n",
    "    questions = list(reader)[:N_QUESTIONS]\n",
    "\n",
    "print(f\"Loaded {len(roles)} roles, {len(questions)} questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize (checkpoint not needed with new sequential flow)\n",
    "completed_roles = set()\n",
    "print(f\"Ready to process {len(roles)} roles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Generate All Responses (vLLM)\n",
    "\n",
    "Generate responses for ALL roles at once, then unload vLLM to free GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vLLM for generation\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "print(f\"Loading vLLM: {MODEL_NAME}\")\n",
    "llm = LLM(\n",
    "    model=MODEL_NAME,\n",
    "    tensor_parallel_size=1,\n",
    "    gpu_memory_utilization=0.9,  # Can use more since HF model loads later\n",
    "    dtype=\"bfloat16\",\n",
    "    max_model_len=1024,\n",
    ")\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    max_tokens=200,\n",
    ")\n",
    "\n",
    "print(\"vLLM loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate ALL responses for ALL roles\n",
    "all_responses = {}  # role_name -> list of {system_prompt, question, response}\n",
    "\n",
    "roles_to_generate = [r for r in roles.keys() if r not in completed_roles]\n",
    "print(f\"Generating responses for {len(roles_to_generate)} roles...\")\n",
    "print(f\"Each role: {N_SYSTEM_PROMPTS} prompts × {N_QUESTIONS} questions = {N_SYSTEM_PROMPTS * N_QUESTIONS} responses\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for role_name in tqdm(roles_to_generate):\n",
    "    role_data = roles[role_name]\n",
    "    \n",
    "    # Get system prompts\n",
    "    if isinstance(role_data, dict) and 'system_prompts' in role_data:\n",
    "        system_prompts = role_data['system_prompts'][:N_SYSTEM_PROMPTS]\n",
    "    elif isinstance(role_data, list):\n",
    "        system_prompts = role_data[:N_SYSTEM_PROMPTS]\n",
    "    else:\n",
    "        system_prompts = [str(role_data)]\n",
    "    \n",
    "    # Generate responses for this role\n",
    "    responses_data = []\n",
    "    for sys_prompt in system_prompts:\n",
    "        prompts = []\n",
    "        for q in questions:\n",
    "            q_text = q['question'] if isinstance(q, dict) else str(q)\n",
    "            chat = f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{sys_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{q_text}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "            prompts.append(chat)\n",
    "        \n",
    "        outputs = llm.generate(prompts, sampling_params)\n",
    "        \n",
    "        for i, output in enumerate(outputs):\n",
    "            responses_data.append({\n",
    "                'system_prompt': sys_prompt,\n",
    "                'question': questions[i],\n",
    "                'response': output.outputs[0].text,\n",
    "            })\n",
    "    \n",
    "    all_responses[role_name] = responses_data\n",
    "\n",
    "print(f\"\\nGenerated {sum(len(v) for v in all_responses.values())} total responses for {len(all_responses)} roles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unload vLLM to free GPU memory\n",
    "del llm\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(f\"vLLM unloaded. GPU memory freed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Judge All Responses (OpenAI API)\n",
    "\n",
    "Score all responses using GPT-4o-mini. Rate limited to 5 RPS for Tier 1 API safety."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup OpenAI for judging\n",
    "import openai\n",
    "import time\n",
    "import re\n",
    "\n",
    "client = openai.OpenAI()\n",
    "\n",
    "# Judge model config\n",
    "JUDGE_MODEL = \"gpt-4o-mini\"  # Cheapest, ~$0.15/1M input tokens\n",
    "JUDGE_MAX_TOKENS = 10\n",
    "\n",
    "# Rate limiting for Tier 1: 500 RPM = ~8 RPS, we use 5 RPS to be safe\n",
    "REQUESTS_PER_SECOND = 5\n",
    "MIN_REQUEST_INTERVAL = 1.0 / REQUESTS_PER_SECOND  # 0.2 seconds\n",
    "\n",
    "# Retry config\n",
    "MAX_RETRIES = 3\n",
    "RETRY_BACKOFF = 2.0\n",
    "\n",
    "last_request_time = 0\n",
    "\n",
    "def judge_response(role_name, system_prompt, question, response):\n",
    "    \"\"\"Score how well a response embodies the role (0-3).\"\"\"\n",
    "    global last_request_time\n",
    "    \n",
    "    prompt = f\"\"\"Score how well this AI response embodies the assigned role (0-3):\n",
    "- 0: Refuses to engage\n",
    "- 1: Acknowledges can't be role but helps\n",
    "- 2: Shows some role traits but breaks character\n",
    "- 3: Fully embodies the role\n",
    "\n",
    "Role: {role_name}\n",
    "System: {system_prompt[:300]}\n",
    "User: {question[:200]}\n",
    "Response: {response[:500]}\n",
    "\n",
    "Score (just the number):\"\"\"\n",
    "    \n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "            # Rate limiting\n",
    "            elapsed = time.time() - last_request_time\n",
    "            if elapsed < MIN_REQUEST_INTERVAL:\n",
    "                time.sleep(MIN_REQUEST_INTERVAL - elapsed)\n",
    "            \n",
    "            last_request_time = time.time()\n",
    "            \n",
    "            result = client.chat.completions.create(\n",
    "                model=JUDGE_MODEL,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                max_tokens=JUDGE_MAX_TOKENS,\n",
    "                temperature=0,\n",
    "            )\n",
    "            \n",
    "            content = result.choices[0].message.content.strip()\n",
    "            match = re.search(r'[0-3]', content)\n",
    "            if match:\n",
    "                return int(match.group())\n",
    "            return 0\n",
    "            \n",
    "        except openai.RateLimitError:\n",
    "            wait_time = RETRY_BACKOFF ** (attempt + 1)\n",
    "            print(f\"  Rate limited, waiting {wait_time}s...\")\n",
    "            time.sleep(wait_time)\n",
    "        except Exception as e:\n",
    "            if attempt == MAX_RETRIES - 1:\n",
    "                return 0\n",
    "            time.sleep(RETRY_BACKOFF ** attempt)\n",
    "    \n",
    "    return 0\n",
    "\n",
    "print(f\"Judging with {JUDGE_MODEL} @ {REQUESTS_PER_SECOND} RPS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Judge ALL responses and collect score=3 responses\n",
    "scored_responses = {}  # role_name -> list of {system_prompt, question, response} with score=3\n",
    "\n",
    "total_responses = sum(len(v) for v in all_responses.values())\n",
    "print(f\"Judging {total_responses} responses...\")\n",
    "print(f\"Estimated time: {total_responses / REQUESTS_PER_SECOND / 60:.1f} minutes\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "total_score_3 = 0\n",
    "for role_name in tqdm(all_responses.keys()):\n",
    "    score_3_data = []\n",
    "    \n",
    "    for resp in all_responses[role_name]:\n",
    "        q_text = resp['question']['question'] if isinstance(resp['question'], dict) else str(resp['question'])\n",
    "        score = judge_response(role_name, resp['system_prompt'], q_text, resp['response'])\n",
    "        if score == 3:\n",
    "            score_3_data.append(resp)\n",
    "    \n",
    "    scored_responses[role_name] = score_3_data\n",
    "    total_score_3 += len(score_3_data)\n",
    "\n",
    "print(f\"\\nJudging complete: {total_score_3} score=3 responses out of {total_responses}\")\n",
    "print(f\"Average score=3 rate: {total_score_3/total_responses*100:.1f}%\")\n",
    "\n",
    "# Free memory - we don't need all_responses anymore\n",
    "del all_responses\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5: Extract Activations (HuggingFace)\n",
    "\n",
    "Load HF model and extract activations for all score=3 responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load HuggingFace model for activation extraction\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "print(f\"Loading HuggingFace model: {MODEL_NAME}\")\n",
    "hf_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "hf_model.eval()\n",
    "\n",
    "hf_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "hf_tokenizer.pad_token = hf_tokenizer.eos_token\n",
    "\n",
    "print(\"HF model loaded!\")\n",
    "\n",
    "def extract_activation(text):\n",
    "    \"\"\"Extract mean activation at all layers for a single text.\"\"\"\n",
    "    inputs = hf_tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    inputs = {k: v.to(hf_model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    activations = []\n",
    "    \n",
    "    def hook(module, input, output):\n",
    "        h = output[0] if isinstance(output, tuple) else output\n",
    "        activations.append(h.mean(dim=1).squeeze(0).detach().cpu())\n",
    "    \n",
    "    handles = [hf_model.model.layers[i].register_forward_hook(hook) for i in range(TOTAL_LAYERS)]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        hf_model(**inputs)\n",
    "    \n",
    "    for h in handles:\n",
    "        h.remove()\n",
    "    \n",
    "    # Clear GPU memory\n",
    "    del inputs\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return torch.stack(activations)  # (n_layers, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract activations for all score=3 responses\n",
    "role_vectors = {}\n",
    "MAX_ACTIVATIONS_PER_ROLE = 20  # Limit per role to save time\n",
    "\n",
    "total_extractions = sum(min(len(v), MAX_ACTIVATIONS_PER_ROLE) for v in scored_responses.values() if len(v) > 0)\n",
    "print(f\"Extracting activations for {total_extractions} responses...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "extraction_count = 0\n",
    "for role_name in tqdm(scored_responses.keys()):\n",
    "    score_3_data = scored_responses[role_name]\n",
    "    \n",
    "    if len(score_3_data) == 0:\n",
    "        print(f\"  Skipping {role_name}: no score=3 responses\")\n",
    "        continue\n",
    "    \n",
    "    activations = []\n",
    "    for i, resp in enumerate(score_3_data[:MAX_ACTIVATIONS_PER_ROLE]):\n",
    "        q_text = resp['question']['question'] if isinstance(resp['question'], dict) else str(resp['question'])\n",
    "        full_text = f\"{resp['system_prompt']}\\n\\nUser: {q_text}\\n\\nAssistant: {resp['response']}\"\n",
    "        \n",
    "        act = extract_activation(full_text)\n",
    "        activations.append(act)\n",
    "        extraction_count += 1\n",
    "        \n",
    "        # Periodic cleanup every 5 extractions\n",
    "        if (i + 1) % 5 == 0:\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # Compute mean vector for this role\n",
    "    role_vectors[role_name] = torch.stack(activations).mean(dim=0)\n",
    "    \n",
    "    # Cleanup\n",
    "    del activations\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"\\nExtracted {extraction_count} activations for {len(role_vectors)} roles\")\n",
    "\n",
    "# Free scored_responses\n",
    "del scored_responses\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unload HF model\n",
    "del hf_model\n",
    "del hf_tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"HF model unloaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 6: Compute Final Axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save role vectors\n",
    "torch.save(role_vectors, ROLE_VECTORS_PATH)\n",
    "print(f\"Saved {len(role_vectors)} role vectors to: {ROLE_VECTORS_PATH}\")\n",
    "\n",
    "# Compute axis: default - mean(roles)\n",
    "DEFAULT_ROLE = 'default'\n",
    "\n",
    "if DEFAULT_ROLE in role_vectors:\n",
    "    default_vec = role_vectors[DEFAULT_ROLE]\n",
    "    character_vecs = {k: v for k, v in role_vectors.items() if k != DEFAULT_ROLE}\n",
    "else:\n",
    "    print(f\"No '{DEFAULT_ROLE}' role found. Using overall mean as default.\")\n",
    "    all_vecs = torch.stack(list(role_vectors.values()))\n",
    "    default_vec = all_vecs.mean(dim=0)\n",
    "    character_vecs = role_vectors\n",
    "\n",
    "# Compute axis\n",
    "character_mean = torch.stack(list(character_vecs.values())).mean(dim=0)\n",
    "axis = default_vec - character_mean\n",
    "\n",
    "print(f\"Axis shape: {axis.shape}\")\n",
    "print(f\"Axis norm at target layer ({TARGET_LAYER}): {axis[TARGET_LAYER].norm():.3f}\")\n",
    "\n",
    "# Save axis\n",
    "torch.save(axis, AXIS_PATH)\n",
    "print(f\"Saved axis to: {AXIS_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"\n",
    "{'='*60}\n",
    "ASSISTANT AXIS COMPUTATION COMPLETE\n",
    "{'='*60}\n",
    "\n",
    "Saved files:\n",
    "  - Axis: {AXIS_PATH} ({os.path.getsize(AXIS_PATH)/1e3:.0f} KB)\n",
    "  - Role vectors: {ROLE_VECTORS_PATH} ({os.path.getsize(ROLE_VECTORS_PATH)/1e6:.1f} MB)\n",
    "\n",
    "Axis details:\n",
    "  - Shape: {axis.shape}\n",
    "  - Model: {MODEL_NAME}\n",
    "  - Target layer: {TARGET_LAYER}\n",
    "  - Roles processed: {len(role_vectors)}\n",
    "\n",
    "Next: Run jailbreak_analysis.ipynb\n",
    "{'='*60}\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
