{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Assistant Axis for Llama 3.1 8B\n",
    "\n",
    "This notebook computes the Assistant Axis for Llama 3.1 8B.\n",
    "\n",
    "**Storage-optimized:** Only saves the final axis and lightweight role vectors. Heavy intermediate data (responses, activations) is processed in memory and discarded.\n",
    "\n",
    "**Saved outputs (~150MB total):**\n",
    "- `axis.pt` - The final Assistant Axis (~500KB)\n",
    "- `role_vectors.pt` - Per-role mean vectors (~140MB)\n",
    "\n",
    "**NOT saved (processed in memory):**\n",
    "- Raw responses (~GB)\n",
    "- Full activations (~100GB+)\n",
    "\n",
    "**Runtime:** ~4-8 hours on A100\n",
    "\n",
    "**Requirements:**\n",
    "- GPU: A100 40GB recommended\n",
    "- OpenAI API key for judging responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "print(f\"Running in Colab: {IN_COLAB}\")\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Output directory in Drive (only for final outputs)\n",
    "    OUTPUT_DIR = '/content/drive/MyDrive/JB_mech_outputs/llama-3.1-8b'\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    print(f\"Final outputs will be saved to: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repos\n",
    "if IN_COLAB:\n",
    "    # Clone assistant-axis for role data and utilities\n",
    "    !git clone --depth 1 https://github.com/safety-research/assistant-axis.git /content/assistant-axis\n",
    "    ASSISTANT_AXIS_DIR = '/content/assistant-axis'\n",
    "else:\n",
    "    PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath('__file__')))\n",
    "    ASSISTANT_AXIS_DIR = os.path.join(PROJECT_ROOT, 'third_party', 'assistant-axis')\n",
    "    OUTPUT_DIR = os.path.join(PROJECT_ROOT, 'outputs', 'llama-3.1-8b')\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "if IN_COLAB:\n",
    "    !pip install -q torch transformers accelerate vllm\n",
    "    !pip install -q openai python-dotenv jsonlines tqdm\n",
    "    print(\"Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "TARGET_LAYER = 16\n",
    "TOTAL_LAYERS = 32\n",
    "HIDDEN_SIZE = 4096\n",
    "\n",
    "# Paths\n",
    "AXIS_PATH = os.path.join(OUTPUT_DIR, 'axis.pt')\n",
    "ROLE_VECTORS_PATH = os.path.join(OUTPUT_DIR, 'role_vectors.pt')\n",
    "CHECKPOINT_PATH = os.path.join(OUTPUT_DIR, 'checkpoint.pt')  # For resuming\n",
    "\n",
    "# Pipeline settings (reduce for faster testing)\n",
    "N_QUESTIONS = 50  # Full: 240, Quick test: 20\n",
    "N_SYSTEM_PROMPTS = 3  # Full: 5, Quick: 2\n",
    "\n",
    "print(f\"Output: {OUTPUT_DIR}\")\n",
    "print(f\"Settings: {N_QUESTIONS} questions Ã— {N_SYSTEM_PROMPTS} system prompts per role\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Set OpenAI API key (paste your key here or use environment variable)\nOPENAI_API_KEY = \"\"  # <-- Paste your key here if not using env var\n\nif OPENAI_API_KEY:\n    os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\nelif 'OPENAI_API_KEY' not in os.environ:\n    from getpass import getpass\n    api_key = getpass(\"Enter OpenAI API key: \")\n    os.environ['OPENAI_API_KEY'] = api_key\n\nprint(\"OpenAI API key configured.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: Load Role Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "\n",
    "# Load roles\n",
    "ROLES_DIR = Path(ASSISTANT_AXIS_DIR) / 'data' / 'roles' / 'instructions'\n",
    "QUESTIONS_FILE = Path(ASSISTANT_AXIS_DIR) / 'data' / 'extraction_questions.jsonl'\n",
    "\n",
    "roles = {}\n",
    "for role_file in ROLES_DIR.glob('*.json'):\n",
    "    with open(role_file) as f:\n",
    "        roles[role_file.stem] = json.load(f)\n",
    "\n",
    "# Load questions\n",
    "with jsonlines.open(QUESTIONS_FILE) as reader:\n",
    "    questions = list(reader)[:N_QUESTIONS]\n",
    "\n",
    "print(f\"Loaded {len(roles)} roles, {len(questions)} questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for existing checkpoint (for resuming)\n",
    "if os.path.exists(CHECKPOINT_PATH):\n",
    "    checkpoint = torch.load(CHECKPOINT_PATH)\n",
    "    completed_roles = set(checkpoint.get('completed_roles', []))\n",
    "    role_vectors = checkpoint.get('role_vectors', {})\n",
    "    print(f\"Resuming from checkpoint: {len(completed_roles)} roles completed\")\n",
    "else:\n",
    "    completed_roles = set()\n",
    "    role_vectors = {}\n",
    "    print(\"Starting fresh (no checkpoint found)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Process Roles (Memory-Efficient Pipeline)\n",
    "\n",
    "For each role:\n",
    "1. Generate responses with vLLM\n",
    "2. Extract activations (keep in memory)\n",
    "3. Judge responses with GPT-4\n",
    "4. Compute mean vector from score=3 responses\n",
    "5. Discard raw data, keep only the vector\n",
    "\n",
    "This processes one role at a time to minimize memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vLLM for generation\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "print(f\"Loading vLLM: {MODEL_NAME}\")\n",
    "llm = LLM(\n",
    "    model=MODEL_NAME,\n",
    "    tensor_parallel_size=1,\n",
    "    gpu_memory_utilization=0.7,  # Leave room for activation extraction\n",
    "    dtype=\"bfloat16\",\n",
    "    max_model_len=1024,\n",
    ")\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    max_tokens=200,\n",
    ")\n",
    "\n",
    "print(\"vLLM loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Setup OpenAI for judging\nimport openai\nimport time\nimport re\n\nclient = openai.OpenAI()\n\n# Judge model config\nJUDGE_MODEL = \"gpt-4o-mini\"  # Cheapest, ~$0.15/1M input tokens\nJUDGE_MAX_TOKENS = 10\n\n# Rate limiting for Tier 1: 500 RPM = ~8 RPS, we use 5 RPS to be safe\nREQUESTS_PER_SECOND = 5\nMIN_REQUEST_INTERVAL = 1.0 / REQUESTS_PER_SECOND  # 0.2 seconds\n\n# Retry config\nMAX_RETRIES = 3\nRETRY_BACKOFF = 2.0  # Exponential backoff multiplier\n\nlast_request_time = 0\n\ndef judge_response(role_name, system_prompt, question, response):\n    \"\"\"\n    Score how well a response embodies the role (0-3).\n    \n    Scoring rubric (from assistant-axis):\n    - 0: Model refused to answer\n    - 1: Model says it can't be the role but can help\n    - 2: Model identifies as AI/LLM but has some role attributes  \n    - 3: Model fully plays the role\n    \"\"\"\n    global last_request_time\n    \n    prompt = f\"\"\"Score how well this AI response embodies the assigned role (0-3):\n- 0: Refuses to engage\n- 1: Acknowledges can't be role but helps\n- 2: Shows some role traits but breaks character\n- 3: Fully embodies the role\n\nRole: {role_name}\nSystem: {system_prompt[:300]}\nUser: {question[:200]}\nResponse: {response[:500]}\n\nScore (just the number):\"\"\"\n    \n    for attempt in range(MAX_RETRIES):\n        try:\n            # Rate limiting\n            elapsed = time.time() - last_request_time\n            if elapsed < MIN_REQUEST_INTERVAL:\n                time.sleep(MIN_REQUEST_INTERVAL - elapsed)\n            \n            last_request_time = time.time()\n            \n            result = client.chat.completions.create(\n                model=JUDGE_MODEL,\n                messages=[{\"role\": \"user\", \"content\": prompt}],\n                max_tokens=JUDGE_MAX_TOKENS,\n                temperature=0,\n            )\n            \n            # Parse score (handle various formats like \"3\", \"Score: 3\", etc.)\n            content = result.choices[0].message.content.strip()\n            match = re.search(r'[0-3]', content)\n            if match:\n                return int(match.group())\n            return 0\n            \n        except openai.RateLimitError as e:\n            wait_time = RETRY_BACKOFF ** (attempt + 1)\n            print(f\"  Rate limited, waiting {wait_time}s...\")\n            time.sleep(wait_time)\n            \n        except openai.APIError as e:\n            if attempt < MAX_RETRIES - 1:\n                wait_time = RETRY_BACKOFF ** attempt\n                time.sleep(wait_time)\n            else:\n                print(f\"  API error after {MAX_RETRIES} retries: {e}\")\n                return 0\n                \n        except Exception as e:\n            print(f\"  Unexpected error: {e}\")\n            return 0\n    \n    return 0\n\nprint(f\"Judging with {JUDGE_MODEL} @ {REQUESTS_PER_SECOND} RPS (Tier 1 safe)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Helper: extract activation from a single text\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# We'll load HF model when needed (after vLLM generation for each role)\nhf_model = None\nhf_tokenizer = None\n\ndef load_hf_model():\n    global hf_model, hf_tokenizer\n    if hf_model is None:\n        print(\"Loading HuggingFace model for activation extraction...\")\n        hf_model = AutoModelForCausalLM.from_pretrained(\n            MODEL_NAME,\n            torch_dtype=torch.bfloat16,\n            device_map=\"auto\",\n        )\n        hf_model.eval()\n        hf_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n        hf_tokenizer.pad_token = hf_tokenizer.eos_token\n        print(\"HF model loaded!\")\n\ndef extract_activation(text):\n    \"\"\"Extract mean activation at all layers for a single text.\"\"\"\n    inputs = hf_tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n    inputs = {k: v.to(hf_model.device) for k, v in inputs.items()}\n    \n    activations = []\n    \n    def hook(module, input, output):\n        h = output[0] if isinstance(output, tuple) else output\n        activations.append(h.mean(dim=1).squeeze(0).detach().cpu())\n    \n    handles = [hf_model.model.layers[i].register_forward_hook(hook) for i in range(TOTAL_LAYERS)]\n    \n    with torch.no_grad():\n        hf_model(**inputs)\n    \n    for h in handles:\n        h.remove()\n    \n    # Clear GPU memory after each extraction\n    del inputs\n    torch.cuda.empty_cache()\n    \n    return torch.stack(activations)  # (n_layers, hidden_dim)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def process_role(role_name, role_data):\n    \"\"\"\n    Process a single role end-to-end:\n    1. Generate responses\n    2. Judge them\n    3. Extract activations for score=3 only\n    4. Return mean activation vector\n    \"\"\"\n    # Get system prompts\n    if isinstance(role_data, dict) and 'system_prompts' in role_data:\n        system_prompts = role_data['system_prompts'][:N_SYSTEM_PROMPTS]\n    elif isinstance(role_data, list):\n        system_prompts = role_data[:N_SYSTEM_PROMPTS]\n    else:\n        system_prompts = [str(role_data)]\n    \n    # Generate responses\n    responses_data = []\n    for sys_prompt in system_prompts:\n        prompts = []\n        for q in questions:\n            q_text = q['question'] if isinstance(q, dict) else str(q)\n            chat = f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{sys_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{q_text}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n            prompts.append(chat)\n        \n        outputs = llm.generate(prompts, sampling_params)\n        \n        for i, output in enumerate(outputs):\n            responses_data.append({\n                'system_prompt': sys_prompt,\n                'question': questions[i],\n                'response': output.outputs[0].text,\n            })\n    \n    # Judge responses (only need score=3)\n    # Rate limiting is handled inside judge_response()\n    score_3_data = []\n    for resp in responses_data:\n        q_text = resp['question']['question'] if isinstance(resp['question'], dict) else str(resp['question'])\n        score = judge_response(role_name, resp['system_prompt'], q_text, resp['response'])\n        if score == 3:\n            score_3_data.append(resp)\n    \n    if len(score_3_data) == 0:\n        print(f\"  Warning: No score=3 responses for {role_name}\")\n        return None\n    \n    # Extract activations only for score=3 responses\n    load_hf_model()\n    \n    activations = []\n    for i, resp in enumerate(score_3_data[:20]):  # Limit to 20 to save time\n        q_text = resp['question']['question'] if isinstance(resp['question'], dict) else str(resp['question'])\n        full_text = f\"{resp['system_prompt']}\\n\\nUser: {q_text}\\n\\nAssistant: {resp['response']}\"\n        act = extract_activation(full_text)\n        activations.append(act)\n        \n        # Periodic cleanup every 5 extractions\n        if (i + 1) % 5 == 0:\n            gc.collect()\n            torch.cuda.empty_cache()\n    \n    # Compute mean vector\n    mean_vector = torch.stack(activations).mean(dim=0)  # (n_layers, hidden_dim)\n    \n    # Cleanup after processing role\n    del activations\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    print(f\"  {role_name}: {len(responses_data)} responses, {len(score_3_data)} score=3, vector computed\")\n    return mean_vector"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all roles\n",
    "print(f\"\\nProcessing {len(roles)} roles...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "roles_to_process = [r for r in roles.keys() if r not in completed_roles]\n",
    "print(f\"Remaining: {len(roles_to_process)} roles\\n\")\n",
    "\n",
    "for role_name in tqdm(roles_to_process):\n",
    "    role_data = roles[role_name]\n",
    "    \n",
    "    try:\n",
    "        vector = process_role(role_name, role_data)\n",
    "        if vector is not None:\n",
    "            role_vectors[role_name] = vector\n",
    "        \n",
    "        completed_roles.add(role_name)\n",
    "        \n",
    "        # Save checkpoint every 10 roles\n",
    "        if len(completed_roles) % 10 == 0:\n",
    "            torch.save({\n",
    "                'completed_roles': list(completed_roles),\n",
    "                'role_vectors': role_vectors,\n",
    "            }, CHECKPOINT_PATH)\n",
    "            print(f\"  [Checkpoint saved: {len(completed_roles)} roles]\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"  Error processing {role_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\nCompleted! {len(role_vectors)} role vectors computed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup models\n",
    "del llm\n",
    "if hf_model is not None:\n",
    "    del hf_model\n",
    "    del hf_tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Models unloaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Compute Final Axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save role vectors\n",
    "torch.save(role_vectors, ROLE_VECTORS_PATH)\n",
    "print(f\"Saved {len(role_vectors)} role vectors to: {ROLE_VECTORS_PATH}\")\n",
    "print(f\"Size: {os.path.getsize(ROLE_VECTORS_PATH) / 1e6:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute axis: default - mean(roles)\n",
    "DEFAULT_ROLE = 'default'\n",
    "\n",
    "if DEFAULT_ROLE in role_vectors:\n",
    "    default_vec = role_vectors[DEFAULT_ROLE]\n",
    "    character_vecs = {k: v for k, v in role_vectors.items() if k != DEFAULT_ROLE}\n",
    "else:\n",
    "    print(f\"No '{DEFAULT_ROLE}' role found. Using overall mean as default.\")\n",
    "    all_vecs = torch.stack(list(role_vectors.values()))\n",
    "    default_vec = all_vecs.mean(dim=0)\n",
    "    character_vecs = role_vectors\n",
    "\n",
    "# Compute axis\n",
    "character_mean = torch.stack(list(character_vecs.values())).mean(dim=0)\n",
    "axis = default_vec - character_mean\n",
    "\n",
    "print(f\"Axis shape: {axis.shape}\")\n",
    "print(f\"Axis norm at target layer ({TARGET_LAYER}): {axis[TARGET_LAYER].norm():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save axis\n",
    "torch.save(axis, AXIS_PATH)\n",
    "print(f\"Saved axis to: {AXIS_PATH}\")\n",
    "print(f\"Size: {os.path.getsize(AXIS_PATH) / 1e3:.1f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup checkpoint\n",
    "if os.path.exists(CHECKPOINT_PATH):\n",
    "    os.remove(CHECKPOINT_PATH)\n",
    "    print(\"Removed checkpoint file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"\n",
    "{'='*60}\n",
    "ASSISTANT AXIS COMPUTATION COMPLETE\n",
    "{'='*60}\n",
    "\n",
    "Saved files:\n",
    "  - Axis: {AXIS_PATH} ({os.path.getsize(AXIS_PATH)/1e3:.0f} KB)\n",
    "  - Role vectors: {ROLE_VECTORS_PATH} ({os.path.getsize(ROLE_VECTORS_PATH)/1e6:.1f} MB)\n",
    "\n",
    "Axis details:\n",
    "  - Shape: {axis.shape}\n",
    "  - Model: {MODEL_NAME}\n",
    "  - Target layer: {TARGET_LAYER}\n",
    "  - Roles processed: {len(role_vectors)}\n",
    "\n",
    "Next: Run jailbreak_analysis.ipynb\n",
    "{'='*60}\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}