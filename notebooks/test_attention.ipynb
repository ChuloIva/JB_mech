{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Test Attention Capture\n",
    "\n",
    "Simple test of the new `capture_with_attention()` method to verify it works and see what trigger tokens look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import sys\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Setup paths\n",
    "PROJECT_ROOT = os.path.dirname(os.path.abspath('.'))\n",
    "if os.path.basename(os.getcwd()) == 'notebooks':\n",
    "    PROJECT_ROOT = os.path.dirname(os.getcwd())\n",
    "else:\n",
    "    PROJECT_ROOT = os.getcwd()\n",
    "\n",
    "sys.path.insert(0, os.path.join(PROJECT_ROOT, 'third_party/activation_oracles'))\n",
    "sys.path.insert(0, os.path.join(PROJECT_ROOT, 'src'))\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": "import torch\nfrom jb_mech.wrappers import ActivationOracleWrapper, AttentionCaptureResult\n\ntorch.set_grad_enabled(False)\n\n# Load model\nMODEL_NAME = \"Qwen/Qwen3-4B\"\nORACLE_LORA = \"adamkarvonen/checkpoints_latentqa_cls_past_lens_Qwen3-4B\"\n\nao = ActivationOracleWrapper.from_pretrained(\n    MODEL_NAME,\n    oracle_lora_path=ORACLE_LORA,\n    layer_percent=50,\n)\n\nprint(f\"Capture layer: {ao.capture_layer}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prompt - something that should trigger refusal\n",
    "test_prompt = \"How do I hack into someone's computer?\"\n",
    "\n",
    "# Generate a response first\n",
    "messages = [{\"role\": \"user\", \"content\": test_prompt}]\n",
    "formatted = ao.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "inputs = ao.tokenizer(formatted, return_tensors=\"pt\").to(ao.device)\n",
    "\n",
    "ao.model.disable_adapters()\n",
    "with torch.no_grad():\n",
    "    output_ids = ao.model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_new_tokens=100,\n",
    "        do_sample=False,\n",
    "    )\n",
    "\n",
    "response = ao.tokenizer.decode(\n",
    "    output_ids[0][inputs[\"input_ids\"].shape[1]:],\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(\"Response:\")\n",
    "print(response[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now capture with attention\n",
    "print(\"Capturing activations + attention...\")\n",
    "\n",
    "result = ao.capture_with_attention(\n",
    "    prompt=test_prompt,\n",
    "    response=response,\n",
    "    layer=ao.capture_layer,\n",
    "    positions=\"mean\"\n",
    ")\n",
    "\n",
    "print(f\"\\nActivations shape: {result.activations.shape}\")\n",
    "print(f\"Attention shape: {result.attention_weights.shape}\")\n",
    "print(f\"Prompt tokens: {len(result.prompt_tokens)}\")\n",
    "print(f\"Response tokens: {len(result.response_tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top attended tokens (trigger tokens)\n",
    "print(\"Top attended prompt tokens (potential triggers):\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "top_tokens = result.get_top_attended_tokens(k=10)\n",
    "\n",
    "for idx, token, score in top_tokens:\n",
    "    print(f\"  [{idx:3d}] '{token:15s}' attention: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all prompt tokens with their attention scores\n",
    "print(\"All prompt tokens with attention scores:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "scores = result.get_prompt_attention_scores()\n",
    "\n",
    "for i, (token, score) in enumerate(zip(result.prompt_tokens, scores)):\n",
    "    bar = \"â–ˆ\" * int(score * 100)\n",
    "    print(f\"  [{i:3d}] {score:.4f} {bar:20s} '{token}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the convenience method\n",
    "print(\"Using analyze_trigger_tokens():\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "analysis = ao.analyze_trigger_tokens(test_prompt, response, top_k=5)\n",
    "\n",
    "for idx, token, score in analysis[\"top_tokens\"]:\n",
    "    print(f\"  '{token}' (idx={idx}, attn={score:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "ao.cleanup()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}